{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 합성곱 신경망 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine 계층 vs. CNN\n",
    "\n",
    "#### Affine 계층\n",
    "- 계층(레이어)간 노드들이 완전 연결되어 있는 신경망 계층\n",
    "\n",
    "<img src=\"picture/affine계층.png\" />\n",
    "\n",
    "- CNN\n",
    "\n",
    "<img src=\"picture/CNN_flow.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 완전연결 계층의 문제점\n",
    "- 데이터의 형상이 무시된다. fully connected 에서는 1차원이어서 그림을 인식하기에 부적합하다..\n",
    "\n",
    "\n",
    "- 입력 데이터가 이미지라면\n",
    "    - 세로\n",
    "    - 가로\n",
    "    - 채녈(색상)\n",
    "    - 공간적 정보: 완전연결 계층에서는 공간적 정보를 무시하고 모든 입력 데이터를 동등한 뉴런으로 취급\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 연산(convolution)\n",
    "- 이미지 처리에서의 필터 연산과 같은 작업\n",
    "\n",
    "<img src=\"picture/convolution.png\" />\n",
    "\n",
    "\n",
    "- 필터의 윈도우(window)를 일정 간격으로 이동해가며 입력 데이터에 적용\n",
    "- 단일 곱셈-누산(fused multiply-add, FMA), 같은위치 곱 후 SUM\n",
    "- 행렬의 내적 곱을 사용하기도 한다.\n",
    "\n",
    "\n",
    "- 특정 맵(feature map)\n",
    "    - CNN에서의 입출력 데이터\n",
    "    \n",
    "    \n",
    "- weight 가 따로 없고, 필터가 weight의 역할을 하게 된다!! \n",
    "\n",
    "\n",
    "- 이미지 processing중 화사하게, 선명하게등의 기능을 위의 필터를 통해서 적용하는 것이다!! \n",
    "\n",
    "\n",
    "- 합성곱 연산의 계산 순서 알지!! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 연산\n",
    "- http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "- 바이어스 고려\n",
    "<img src=\"picture/convolution_with_bias.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습\n",
    "- 필터의 매개변수가 다른 신경망의 가중치에 해당\n",
    "<img src=\"picture/학습.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 후 값을 통해서 세로선, 가로선, 블럭 들을 찾게 해준다. \n",
    "- CNN은 필터가 학습이 되는거다!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패딩(padding)\n",
    "- convolution(합성곱 연산)을 수행하기 전에 입력 데이터 주변을 특정 값으로 채우는 작업\n",
    "\n",
    "- 폭 1짜리 패딩\n",
    "<img src=\"picture/padding.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패딩 사용 목적\n",
    "- 출력 크기 조정\n",
    "\n",
    "- (4,4) 입력 데이터에 (3,3) 필터 적용\n",
    "    - (2,2) 출력\n",
    "    - convolution을 여러 번 적용 -> 출력 크기가 1 -> convolution 적용 불가\n",
    "    \n",
    "- (4,4) 입력 데이터에 패딩 폭1 -> (6,6) 입력 데이터\n",
    "    - (3,3) 필터 적용 -> (4,4) 출력 데이터\n",
    "    \n",
    "- 패딩을 통해서 convolution이 여러변 가능하도록!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스트라이드(stride)\n",
    "- 필터 적용하는 위치 간격\n",
    "- 스트라이드가 2인 경우\n",
    "<img src=\"picture/stride.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스트라이드, 패딩과 출력의 관계\n",
    "- 스트라이드↑, 출력↓\n",
    "- 패딩↑, 출력↓\n",
    "\n",
    "$OH = \\frac{H+2P-FH}{S} +1 $ - 높이   \n",
    "$OW = \\frac{W+2P-FW}{S} +1 $ - 폭    \n",
    "\n",
    "- 입력: (4,4), 패딩:1, 스트라이드:1, 필터:(3,3)\n",
    "    - 출력은?\n",
    "- 입력:(7,7), 패딩:0, 스트라이드:2, 필터:(3,3)\n",
    "- 입력:(28,31), 패딩:2, 스트라이드:3, 필터:(5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3차원 데이터의 합성곱 연산\n",
    "- 채널까지 고려한 이미지 데이터: 3차원 데이터\n",
    "- 필터도 3차원: 입력데이터의 채널 수와 필터의 채널 수가 일치해야 함\n",
    "- **출력 데이터는 2차원**\n",
    "<img src=\"picture/3convolution.png\" />\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"picture/3convolution_cal.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 블록으로 생각하기\n",
    "- 데이터와 필터를 직육면체 블록으로 간주\n",
    "- (채널, 높이, 너비) 순서로 표기\n",
    "    - (C, H, W)\n",
    "    - (C, FH, FW)\n",
    "    \n",
    "  \n",
    "<img src=\"picture/3convolution3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 픽터의 개수 -> 출력 특징 맵 개수\n",
    "- 필터의 가중치 데이터 : 4차원 데이터\n",
    "    - (20, 3, 5, 5)\n",
    "    \n",
    "### 여러 필터를 사용한 합성곱 연산\n",
    "\n",
    "<img src=\"picture/3convolution_cal2.png\" />\n",
    "\n",
    "- 필터 FN개로 출력 FN개 뽑음!\n",
    "- 필터가 4차원 이라고 하네!! (FH C FW FN) 으로\n",
    "- 필터의 값을 변경하는 것이 학습하는 거다!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 바이어스가 추가된 합성곱 연산의 처리 흐름\n",
    "- 채널 당 바이어스 하나 할당\n",
    "<img src=\"picture/convolution_with_bias3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 처리\n",
    "- 각 신경망 계층을 흐르는 데이터 : 4차원\n",
    "    - (데이터 수, 채널 수, 높이, 너비)\n",
    "- N 데이터\n",
    "<img src=\"picture/convolution_batch.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀링 계층(pooling layer)\n",
    "- 풀링\n",
    "    - 세로 및 가로 방향의 공간을 줄이는 연산\n",
    "    - 최대(max) 풀링, 평균(average) 풀링 / 최대풀링을 더 많이 씀, 연산이 적어서\n",
    "\n",
    "\n",
    "- 최대 풀링(max pooling)을 스트라이드 2로 처리하는 예\n",
    "\n",
    "<img src=\"picture/pooling.png\" />\n",
    "\n",
    "\n",
    "- ReLU 후에 Pooling을 하네, 선택적이지. \n",
    "- 한 공간에 빨간색만 많이 있다 치면 그것을 다 연산하는 것보다 Pooling 해서 연산하는 것이 좋지!! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀링 계층의 특징\n",
    "1. 학습해야 할 매개변수가 없다.\n",
    "    - 최댓값 혹은 평균을 취하는 명확한 처리하므로\n",
    "    \n",
    "2. 채널 수가 변하지 않는다.\n",
    "    - 채널마다 독립적으로 계산하므로\n",
    "\n",
    "<img src=\"picture/pooling_feature.png\" />\n",
    "\n",
    "\n",
    "3. 입력의 변화에 강건하다(영향을 적게 받는다) - Robust 하다!\n",
    "    - 입력 데이터가 조금 변해도 풀링의 결과는 잘 변하지 않는다.\n",
    "    \n",
    "<img src=\"picture/pooling_feature2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolution 계층 및 풀링 계층 구현하기\n",
    "- 4차원 배열\n",
    "\n",
    "### Im2col 함수\n",
    "- 4차원 배열의 입력 데이터를 필터링(가중치 계산)하기 좋게 전개하는 함수\n",
    "- batch data\n",
    "\n",
    "<img src=\"picture/im2col.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1차원으로 펼쳐버리네..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 데이터에서 필터를 적용하는 영역(3차원 블록)을 한 줄로 전개\n",
    "<img src=\"picture/im2col2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 필터를 적용하기 전 준비하는 단계이다!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컨벌루션 연산의 필터 처리 상세과정\n",
    "- 필터를 세로로 1열로 전개\n",
    "- Im2col이 전개한 데이터와 1열로 전개한 필터를 행렬 내적 연산\n",
    "- 출력 데이터를 변형(reshape)\n",
    "\n",
    "<img src=\"picture/im2col_detail3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 중첩 for loop를 쓰지 않기 위해서 차원을 늘려서 행렬연산으로 해결!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 계층 구현하기\n",
    "- im2col 함수의 인터페이스\n",
    "    - common/util.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "im2col(input_data, filter_h, filter_w, stride=1, pad=0) # \n",
    "    * input_data: (데이터 수, 채널 수, 높이, 너비)의 4차원 배열"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### im2col 함수 사용예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from origin.common.util import im2col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = np.random.rand(1,3,7,7) # (데이터수, 채널 수, 높이, 너비)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 7, 7)\n",
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "print(x1.shape)\n",
    "print(col1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1표모음 3표 7행 7열\n",
    "- 9행 75열\n",
    "<img src=\"picture/im2col_practice.jpg\" />\n",
    "<img src=\"picture/im2col_practice2.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.87777875  0.67228848  0.23722382  0.27481371  0.33299883]\n",
      "   [ 0.27802886  0.18626135  0.45132356  0.05330373  0.78401352]\n",
      "   [ 0.15036712  0.07681448  0.35860239  0.37308431  0.02348845]\n",
      "   [ 0.79654802  0.06232607  0.1584338   0.42041003  0.56258361]\n",
      "   [ 0.42921962  0.79963761  0.81248993  0.78268385  0.48903496]]\n",
      "\n",
      "  [[ 0.56272886  0.3149854   0.83369376  0.16627463  0.02169535]\n",
      "   [ 0.76339882  0.29131283  0.6204335   0.95618729  0.30715597]\n",
      "   [ 0.45541981  0.65162417  0.36126868  0.59990628  0.46762084]\n",
      "   [ 0.35459288  0.48994009  0.07943951  0.68304741  0.19321725]\n",
      "   [ 0.67105399  0.92022149  0.73072464  0.84869123  0.75404707]]\n",
      "\n",
      "  [[ 0.71456275  0.3861099   0.48589993  0.48680978  0.76877192]\n",
      "   [ 0.04174912  0.82381177  0.21777485  0.89279246  0.2282715 ]\n",
      "   [ 0.05728496  0.83489447  0.53928142  0.54618851  0.34796117]\n",
      "   [ 0.05403371  0.82149839  0.42127543  0.7453959   0.09043367]\n",
      "   [ 0.76912582  0.04997285  0.17932384  0.1689475   0.77111354]]]]\n"
     ]
    }
   ],
   "source": [
    "print(x1[:, :, :5,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑ 필터가 처음으로 적용될 (3,5,5) = (채널,행,열) 을 뽑아 낸거다!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.87777875,  0.67228848,  0.23722382,  0.27481371,  0.33299883,\n",
       "        0.27802886,  0.18626135,  0.45132356,  0.05330373,  0.78401352,\n",
       "        0.15036712,  0.07681448,  0.35860239,  0.37308431,  0.02348845,\n",
       "        0.79654802,  0.06232607,  0.1584338 ,  0.42041003,  0.56258361,\n",
       "        0.42921962,  0.79963761,  0.81248993,  0.78268385,  0.48903496,\n",
       "        0.56272886,  0.3149854 ,  0.83369376,  0.16627463,  0.02169535,\n",
       "        0.76339882,  0.29131283,  0.6204335 ,  0.95618729,  0.30715597,\n",
       "        0.45541981,  0.65162417,  0.36126868,  0.59990628,  0.46762084,\n",
       "        0.35459288,  0.48994009,  0.07943951,  0.68304741,  0.19321725,\n",
       "        0.67105399,  0.92022149,  0.73072464,  0.84869123,  0.75404707,\n",
       "        0.71456275,  0.3861099 ,  0.48589993,  0.48680978,  0.76877192,\n",
       "        0.04174912,  0.82381177,  0.21777485,  0.89279246,  0.2282715 ,\n",
       "        0.05728496,  0.83489447,  0.53928142,  0.54618851,  0.34796117,\n",
       "        0.05403371,  0.82149839,  0.42127543,  0.7453959 ,  0.09043367,\n",
       "        0.76912582,  0.04997285,  0.17932384,  0.1689475 ,  0.77111354])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(col1[0, :])\n",
    "col1[0, :]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- print(x1[:, :, :5,:5])\n",
    "0.87777875,  0.67228848,  0.23722382,  0.27481371,  0.33299883 ...\n",
    "- col1[0, :]\n",
    "0.87777875,  0.67228848,  0.23722382,  0.27481371,  0.33299883 ...\n",
    "- 값이 똑같이 들어가있는 것을 볼 수 있다. 차원 줄여 준거지!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convolution code 봄 in common/layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 중간 데이터（backward 시 사용）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고, numpy의 reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(np.arange(48)); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
       "       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n",
       "       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
       "       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape((4,-1)) # row 4개 나머지는 알아서!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can only specify one unknown dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ee2198351f0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 이 경우는 불가능 하지!! -1은 1번만 써야한다!!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: can only specify one unknown dimension"
     ]
    }
   ],
   "source": [
    "x.reshape((2,-1, -1)) # 이 경우는 불가능 하지!! -1은 1번만 써야한다!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
       "        [16, 17, 18, 19, 20, 21, 22, 23]],\n",
       "\n",
       "       [[24, 25, 26, 27, 28, 29, 30, 31],\n",
       "        [32, 33, 34, 35, 36, 37, 38, 39],\n",
       "        [40, 41, 42, 43, 44, 45, 46, 47]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape((2,-1, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7]],\n",
       "\n",
       "        [[ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]],\n",
       "\n",
       "\n",
       "       [[[24, 25, 26, 27],\n",
       "         [28, 29, 30, 31]],\n",
       "\n",
       "        [[32, 33, 34, 35],\n",
       "         [36, 37, 38, 39]],\n",
       "\n",
       "        [[40, 41, 42, 43],\n",
       "         [44, 45, 46, 47]]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape((-1,3,2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = np.array(np.arange(27)).reshape(1,3,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [11],\n",
       "       [12],\n",
       "       [13],\n",
       "       [14],\n",
       "       [15],\n",
       "       [16],\n",
       "       [17],\n",
       "       [18],\n",
       "       [19],\n",
       "       [20],\n",
       "       [21],\n",
       "       [22],\n",
       "       [23],\n",
       "       [24],\n",
       "       [25],\n",
       "       [26]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.reshape(1,-1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3x3x3 짜리 필터를 2개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = np.array(np.arange(54)).reshape(2,3,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26],\n",
       "       [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
       "        44, 45, 46, 47, 48, 49, 50, 51, 52, 53]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.reshape(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 27],\n",
       "       [ 1, 28],\n",
       "       [ 2, 29],\n",
       "       [ 3, 30],\n",
       "       [ 4, 31],\n",
       "       [ 5, 32],\n",
       "       [ 6, 33],\n",
       "       [ 7, 34],\n",
       "       [ 8, 35],\n",
       "       [ 9, 36],\n",
       "       [10, 37],\n",
       "       [11, 38],\n",
       "       [12, 39],\n",
       "       [13, 40],\n",
       "       [14, 41],\n",
       "       [15, 42],\n",
       "       [16, 43],\n",
       "       [17, 44],\n",
       "       [18, 45],\n",
       "       [19, 46],\n",
       "       [20, 47],\n",
       "       [21, 48],\n",
       "       [22, 49],\n",
       "       [23, 50],\n",
       "       [24, 51],\n",
       "       [25, 52],\n",
       "       [26, 53]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.reshape(2,-1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 27)\n",
      "(27, 2)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(np.arange(108)).reshape(-1,27)\n",
    "f2 = f.reshape(2,-1)\n",
    "print(x.shape)\n",
    "print(f2.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6201,  15678],\n",
       "       [ 15678,  44838],\n",
       "       [ 25155,  73998],\n",
       "       [ 34632, 103158]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.dot(x,f2.T)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ↑위 결과는 입력 1개에 필터 2개를 적용한 결과. \n",
    "- 1번 column은 x에 대해서 1번 필터를 convolution한 결과! \n",
    "\n",
    "\n",
    "- 위 결과를 다시 입력을 다시 reshape 해야 한다. 2차원으로 filter 개수가 2개 이기 때문에!! \n",
    "    - 행 4가 OH, OW 로 나뉘어야 하고, 열2가 FH가 되어야 한다. \n",
    "    - (4,2) -> (2,2,2) 로 바꾸야 한다!! \n",
    "    \n",
    "- 지금까지 convolution 코드 분할해서 본거다!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  6201,  15678],\n",
       "         [ 15678,  44838]],\n",
       "\n",
       "        [[ 25155,  73998],\n",
       "         [ 34632, 103158]]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.reshape(1,2,2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  6201,  15678],\n",
       "         [ 25155,  34632]],\n",
       "\n",
       "        [[ 15678,  44838],\n",
       "         [ 73998, 103158]]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.reshape(1,2,2,-1).transpose(0,3,1,2) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ↑ 위와 같이 변형해 주었다. \n",
    "- transpose(0,3,1,2) 는 \"표모음=>표모음\"으로 \"표=>열\"로 \"행=>표\"로 \"열=>행\"으로.!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 계층 구현하기\n",
    "- common/layers.py\n",
    "- Convolution 클래스\n",
    "    - forward() 매소드\n",
    "        - im2col 함수 이용하여 입력 데이터 전개\n",
    "        - 필터 전개\n",
    "        - 전개한 입력 데이터와 필터의 행렬 내적 연산\n",
    "        - 출력 데이터의 형상을 재구성하고 축의 순서 변경\n",
    "        \n",
    "        \n",
    "<img src=\"picture/transpose.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Number, Channel, Height, Width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### CNN에도 당연히 역전파가 있다!! \n",
    "    - 계산 그래프 그리며 하면 도움이 된다.\n",
    "    - col2im 있다. 역으로 가야하니!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 계층 구현하기\n",
    "- common/layers.py\n",
    "- Convolution 클래스\n",
    "    - backward() 메소드: 역전파\n",
    "        - Affine 계츠의 구현과 많이 유사\n",
    "        - im2col을 역으로 처리해야 함\n",
    "        - col2im 함수: common/util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "- 역전파 부분 그냥 반대로 해주면 되지!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀링 계층 구현하기\n",
    "- im2col 함수를 사용해 입력 데이터 전개\n",
    "- 채널이 독립인 점이 합성공 계층과 다른 점\n",
    "\n",
    "<img src=\"picture/im2col_pooling.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀링 계층 구현하기\n",
    "- 풀링 계층 구현의 흐름: 최대 풀링(가장 큰 원소는 회색으로 표시)\n",
    "    - 입력 데이터 전개\n",
    "    - 행별 최댓값\n",
    "    - 적절한 모향으로 성형\n",
    "    \n",
    "    \n",
    "<img src=\"picture/complete_pooling.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- common/layers.py\n",
    "- Pooling 클래스\n",
    "    - forward() 메소드\n",
    "        - 입력 데이터 전개\n",
    "        - 행별 최댓값\n",
    "        - 적절한 모영으로 성형\n",
    "    - backward() 메소드\n",
    "        - ReLU 노드 구현 시 사용한 max의 역전파 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 구현하기\n",
    "- 손글씨 숫자를 인식하는 CNN\n",
    "<img src=\"picture/cnn.png\" />\n",
    "- ch07/Simple_convnet.py\n",
    "\n",
    "- ReLU를 왜 사용하나?! \n",
    "    - => 활성화 함수라는 것을 인지하고 있으면 된다. Weight가 0이 되는 경우는 매우 드물다. 그래서 어떤 경우에도 다음 노드에 영향을 미치고 있는 것이다. 그런데 이런 것을 방지하기 위해서, 신호를 보내지 않는 것을 표현하기 위해서 ReLU로 음수를 0으로 전환하는 것이다.\n",
    "    - 이렇게 0을 만드는 것이 Sigmoid보다 ReLU가 효율적이다!\n",
    "    - 0 으로 처리하면 속도도 빨라지고!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax [3층 신경망]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1 # FH, FW\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        # 위는 filter다!! 4차원 배열을 랜덤하게 만듦. (30,28,5,5)\n",
    "        \n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        # 1은닉층\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        \n",
    "        # 2은닉층\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        # 마지막 결과 출력까지!! \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x) # 이 forward라는 method를 다 적용해 두었기 때문에 가능!! \n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 시각화학\n",
    "- CNN의 필터 시각화\n",
    "    - ch07/visualize_filter,.py\n",
    "    \n",
    "<img src=\"picture/cnn_picture.png\" />    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\pydev\\Machine_Learning\\MLiA_py3\\DeepLearning\\origin\\ch07\n"
     ]
    }
   ],
   "source": [
    "%cd origin/ch07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEjCAYAAABD3BobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHGVJREFUeJzt3GtwleW99/F/EiBhJStHQgATTkVUJFQEVChaKZ6Qqghi\nnQoCdqaMokXF0kEBGU8ILzhZB6z1gCgDVgQtJxGPLRYFRzlVDgUJhgBJJCcgCQnczwuutXb2nl2v\n373naZ9tnu/n1f3id/257pV75cfKzLoSgiAwAABglvj/egMAAPxvQSkCAOBQigAAOJQiAAAOpQgA\ngEMpAgDgUIoAADiUIgAADqUIAIDTIkw4MzMz6NChgzfXsmVLeWZVVZWUa926tTyzvr7emykrK7Oa\nmpoEM7O0tLQgOzvbu+bMmTPyHk6ePCnlOnbsKM+sq6uTcvv27SsPgiA3EokEmZmZ3nyYU42U19bM\nrEUL/dFKTk6WcsXFxeVBEOSamUUikSA9Pd275uzZs/I+MjIypNw//vEPeWa3bt28mdLSUquqqkow\nM8vOzg7y8/O9a8rLy+U9nD59Wsp17txZnrl3714pV1NTUx4EQW6bNm0CZb76+yCM4uJiOXveeedJ\nuf3794d+FpOSkuR9qM9tYqL+uSYajXozR48etcrKygQzs9TU1CArK0tao1LmuX9bntmqVSspF/u9\n6MuFKsUOHTrY66+/7s21bdtWnrlhwwYpd+GFF8oz9+/f781Mnz49fp2dnW2TJ0/2rqmoqJD3sHXr\nVik3f/58eeaePXuk3A033FBkZpaZmWnjx4/35hsaGuQ97Nu3T8rl5nqfvbiuXbtKuUmTJhXFrtPT\n023MmDHeNTU1NfI+hgwZIuVuvvlmeeaCBQu8md/85jfx6/z8fFu9erV3zcsvvyzv4cCBA1Ju8eLF\n8syf/exnUu7DDz8sMjtXuMp7Yt26dfIeVJMmTZKzM2fOlHLDhg0L/SympaXJ+6itrZVykUhEnqn8\nzO6+++74dVZWlt1///3eNc8884y8hxEjRki5fv36yTPVDxbXXXddkT/Fn08BAIijFAEAcChFAAAc\nShEAAIdSBADAoRQBAHAoRQAAHEoRAAAn1Jf3jx8/bsuWLfPmCgsL5Znt27eXcnPmzJFnKgcMzJ07\nN37dtm1bu++++7xrHn/8cXkP3bt3l3Jhvji/atUqOWt27gvAX331lTc3ePBgeaaaDXN6x1VXXSXl\nmn4JOz093a655hrvmm3btsn7UL84vnnzZnmm8uXqpidy7Nixwzp16uRd88orr8h7UA8bUA4aiFFP\nEYkpKSmxGTNmeHO9evWSZz766KNSbsqUKfLMnj17ytmY/Px8mzVrljcX5ov26sk6v/3tb+WZGzdu\n9GaaHnbRqlUraR9hTkJSX98nnnhCnllUJH0nX8YnRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAA\nh1IEAMChFAEAcChFAAAcShEAACfUMW+tW7eWjukZNWqUPFM9uqzpEV8+J0+e9GbOnj0bv66trbWd\nO3d615SWlsp7eO6556Rc79695ZnTp0+XcosWLTIzs8rKSun1DYJA3kNFRYWUu/vuu+WZDzzwgJyN\nOX78uL322mveXJijtfbu3SvlLr/8cnnmzJkzvZmqqqr4dY8ePaSjFD/99FN5D+pz26FDB3nm8OHD\npdy7775rZueO/cvIyPDmly9fLu/hkksukXIfffSRPHPq1KlyNubAgQN2xx13eHPqkZZmZi1aaL+a\nGxsb5ZlbtmzxZpr+7iwvL7fFixd71+Tn58t7UI7TNDO79NJL5ZkTJkyQcsqRl2Z8UgQAII5SBADA\noRQBAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAAJ9SJNidPnrTPP//cm7vrrrvkmUuXLpVy\nYU6TmTdvnjdTVFQUv66trZVOOygsLJT3MGTIECmnnuhjZtIJLk316tXL1q9f780pp1bEjBw5Usqt\nXr1antm5c2c5G5ORkWE33XSTN1dWVibPvOiii6RcmBObiouLvZmmJ9pUVVXZmjVrvGvCnGSydu1a\nKXfq1Cl5pvpaxUQiEen0pry8PHlmbm6ulFNOCIp5+OGHpdzEiRPj1xUVFdJJPEOHDpX3MX78eCm3\nbt06eWbr1q29mcTE//ic1KFDB3vssce8a3bt2iXvQX3GwpxgFo1G5ayCT4oAADiUIgAADqUIAIBD\nKQIA4FCKAAA4lCIAAA6lCACAQykCAOBQigAAOJQiAABOqGPe8vLy7KGHHvLm+vXrJ89UjwgaPHiw\nPHPy5MnezPbt2+PXtbW1tnPnTu+a4cOHy3v47LPPpNyll14qz3zxxRflrJnZt99++5+Oo/pn2rRp\nI89UjsMzC3fMW//+/eVsTE1NjX388cfeXN++feWZWVlZUm7lypXyzIKCAm9mx44d8euysjJ74YUX\nvGtuv/12eQ/vvPOOlJs6dao8Mz8/X86amTU0NNiRI0e8uT59+sgzjx49KuVycnLkmcr75b+66KKL\npOMqleMnY2bPni3ltm3bJs8cN26cN9OyZcv4dWJioqWlpXnXfPnll/IelixZIuVmzJghz1y4cKGc\nVfBJEQAAh1IEAMChFAEAcChFAAAcShEAAIdSBADAoRQBAHAoRQAAHEoRAAAnIQgCPZyQUGZmRf+6\n7fxbdQqCINes2d2Xmbu35npfZs3uZ9Zc78uMZ/GHprnel1mTe/s+oUoRAIDmjD+fAgDgUIoAADiU\nIgAADqUIAIBDKQIA4FCKAAA4lCIAAA6lCACAQykCAOBQigAAOJQiAAAOpQgAgEMpAgDgUIoAADiU\nIgAADqUIAIBDKQIA4FCKAAA4lCIAAE6LMOHMzMygXbt23lxFRYU8MyUlRcodOnRInllYWOjNFBcX\n2/HjxxPMzt1X+/btvWvC3NfZs2elXHJysjyzrq5OypWXl5cHQZDbunXrIBqNevNJSUnyHs6cOSPl\nKisr5ZlZWVlSrrS0tDwIglwzs/T09KBt27beNZmZmfI+jhw5IuVqamrkmcpre+rUKauvr08wM0tN\nTQ2U16OkpETeQ2Ki9n/fjIwMeWaLFtqvjtjPLCUlJUhLS/PmO3fuLO+hrKxMyqnPl5nZwYMHpVxV\nVVX8WUxNTQ2U5+z06dPyPtLT06VcmPeu8txWVVXZqVOn4s9idna2d015ebm8B/U1SE1NlWeGeD/G\nf2bfJ1QptmvXzv7whz94cytWrJBnXnjhhVLu3nvvlWeuXr3am/n5z38ev27fvr29/PLL3jVvvfWW\nvIcTJ05IufPPP1+euWfPHin3/PPPF5mZRaNRu+2227x59Q1opt9XmNfq9ttvl3Lz588vil23bdvW\nZs+e7V0zfPhweR9PPvmklPvggw/kmcovyw8//DB+nZWVZRMmTPCuefzxx+U9qP/xvP766+WZeXl5\nUm7evHlFZmZpaWl20003efPK+zBm0aJFUk55D8SMHTtWyq1Zsyb+LGZmZtr48eO9aw4fPizv49pr\nr5VyYd67H330kTfT9PXPzs62Bx54INQaH/U/HVdccYU887333lOjRf4Ifz4FACCOUgQAwKEUAQBw\nKEUAABxKEQAAh1IEAMChFAEAcChFAACcUF/eb9mypRUUFHhzTz31lDxz586dUm7cuHHyTOU0laYn\ns6SmpkpfFu3fv7+8h1tvvVXKDRgwQJ557NgxOWtm1tjYKJ36EebUlyuvvFLKLVmyRJ6pnrjSVEND\ng/R6hPmSe9euXaWccqpTzMiRI72Z7du3x68jkYj16dPHuybMAQItW7aUcps3b5Zn3n///XLW7NwB\nAj169PDmwhz6oD6306ZNk2f26tVLyq1ZsyZ+3b59e5s+fbp3TZhTiIYMGSLlHn30UXmmcmpX02el\ntLTUnn32We8a9Qv5ZvrvceWggRj1uVUPBOCTIgAADqUIAIBDKQIA4FCKAAA4lCIAAA6lCACAQykC\nAOBQigAAOJQiAAAOpQgAgBPqmLfq6mpbv369N3fxxRfLMxMSEqTcSy+9JM+89957vZmmx58VFRXZ\n+PHjvWvuuusueQ9ffPGFlOvZs6c88/Tp03LWzCw5Odl+9KMfeXPKsXgxKSkpUm7EiBHyzKFDh0q5\nuXPnxq8TExMtEol410SjUXkfo0ePlnKXXXaZPHPx4sXeTHl5efw6PT3drr32Wu+a+fPny3vo27ev\nlOvUqZM8c+PGjVLummuuMTOzpKQky8jI8Oazs7PlPahHwv3kJz+RZ3bo0EHKzZw5M359+vRpKy4u\n9q4J8ztMfU+cf/758szS0lI5a3bueViwYEGoNT5FRUVSTvl9HDNlypT/6Xb+W3xSBADAoRQBAHAo\nRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAAh1IEAMAJdaJNcnKyde3a1ZsLcyrFRx99JOUaGhrk\nmU1PCPlnGhsb49fqKSJhXHHFFVLujTfekGf27t071B7q6+tt//793lzHjh3lmeppRQUFBfLMxMTw\n/zf77rvv7NVXX/XmHnzwQXnmwoULpVz//v3lmU1PPvlnmt7/3r177brrrvOuCXOKx8CBA6Xcvn37\n5JndunWTs2ZmZ86csYqKCm/u+PHj8syxY8dKuTAnuQwePFjOxhQVFdmvf/1rb65NmzbyzCVLlki5\n5557Tp45bNgwb+bUqVPx6+rqanv//fe9ax577DF5D998842Ua3p6lY9yapeZ2Ycffijl+KQIAIBD\nKQIA4FCKAAA4lCIAAA6lCACAQykCAOBQigAAOJQiAAAOpQgAgEMpAgDghDrmLTEx0aLRqDe3bNky\neWZhYaGUC3Mc2uzZs72Zm2++OX4dBIHV1dV517z11lvyHoIgkHJ5eXnyzD179shZM7OKigp78803\nvTl1r2Zmhw4dknJ//OMf5Zk9e/aUszE5OTk2evRob+6FF16QZw4ZMkTKzZo1S54ZiUS8mabHvEWj\nUfvpT3/qXVNSUiLv4cYbb5Ryffv2lWeqx3XFRKNRGzRokDf397//XZ6pHGFoZnbhhRfKM//0pz9J\nuZEjR8av1Xu799575X10795dym3cuFGeecstt3gzmzZtil+fPHnStm7d6l0zYsQIeQ9dunSRcmvX\nrpVnDh8+XM4q+KQIAIBDKQIA4FCKAAA4lCIAAA6lCACAQykCAOBQigAAOJQiAAAOpQgAgJMQ5jST\nhISEMjMr+tdt59+qUxAEuWbN7r7M3L011/sya3Y/s+Z6X2Y8iz80zfW+zJrc2/cJVYoAADRn/PkU\nAACHUgQAwKEUAQBwKEUAABxKEQAAh1IEAMChFAEAcChFAAAcShEAAIdSBADAoRQBAHAoRQAAHEoR\nAACHUgQAwKEUAQBwKEUAABxKEQAAh1IEAMChFAEAcFqECaekpARpaWneXPv27eWZDQ0NUk75d2MO\nHjzozZw4ccLq6uoSzMwikUiQkZHhXVNbWyvvIRKJSLns7Gx5ZklJiZSrqKgoD4IgNzs7OygoKPDm\nq6ur5T1kZWVJuS+//FKeqT4vR44cKQ+CINfMLCcnR7q3Y8eOyfto0UJ7O5SWlsoz8/PzvZmysjKr\nrq5OMDNLTk4OlGcnNzdX3sPx48elnPIeiGndurWU27VrV3kQBLkpKSlBamqqN9+qVSt5D0EQSLnT\np0/LM5U9mpkVFxfHn8WEhIQgMdH/+UKdbab/LCorK+WZyutVX19vDQ0NCWbn7kuZ27FjR3kP6u9x\n9fkyMzty5IiUKykpif/Mvk+oUkxLS7OhQ4d6c9OnT5dnqr+0BgwYIM/81a9+5c28/fbb8euMjAwb\nO3asd83OnTvlPfTu3VvKjRo1Sp45bdo0KffGG28UmZkVFBTY+vXrvfmNGzfKe7j11lulXDQalWcq\nPy8zsyeffLIodl1QUGAbNmzwrpkzZ468jzZt2ki5Z599Vp45a9Ysb+Z3v/td/DoSidigQYO8a+65\n5x55D8uWLZNyQ4YMkWdefPHFUq5Hjx5FZucK4cYbb/Tmlf/oxNTX10u5Q4cOyTP79+8v5SZNmhR/\nFhMTE6X/BF9++eXyPq6//nopt3r1anmm8nrt2LFDnhczZcoUOTtw4EAp17NnT3nmk08+KeWmTZtW\n5E/x51MAAOIoRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAAJ9T3FLOysuwXv/iFNxfmC9Pq94JW\nrVolzwwrKyvLbrvtNm/umWeekWcWFxdLuTDfx9m2bZucNTv3pe2lS5d6c4WFhfLMN998U8q99NJL\n8szzzz9fyjX9PlJJSYnNmDHDuybMl4AffvhhKffBBx/IM1esWOHNVFRUxK+TkpKk73ju2bNH3oN6\n6MTRo0flme+9956cNTPLycmxO++805sL813J3bt3S7nBgwfLM9WDDpqKRCLWp08fb27v3r3yzLy8\nPClXVVUlz9y3b5830/RZueiii+y1117zrlEPFTEzmzt3rpRLTk6WZ6rfnVbxSREAAIdSBADAoRQB\nAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAAh1IEAMAJdcxbbW2tbd++3ZvLz8+XZ/75z38O\nswXJBRdc4M2kpKTEr9Vjmr766it5DxMmTJByp06dkmeqr1X37t3N7Nw9Kq/Fjh075D1MmjRJyi1Z\nskSe+dhjj8nZmMbGRulIrilTpsgz1WP0HnnkEXmmclzXzp0749fRaFQ6lqyurk7ew1NPPSXl1GfW\nzOySSy6Rs2bnjn6cN2+eNzd27Fh55sSJE6Wc+sya6cerNT3qr0WLFpadne1d88knn8j7UH4fmZmd\nOHFCntmvXz9vZuXKlfFr9T323XffyXtQj3Ts2rWrPDPMUXcKPikCAOBQigAAOJQiAAAOpQgAgEMp\nAgDgUIoAADiUIgAADqUIAIBDKQIA4IQ60SYvL086HeLTTz+VZx47dkzKDRgwQJ4ZJmtm9u2339pD\nDz0k5VSLFy+WcsqJETFz586Vs2ZmJ0+etC1btnhzBw4ckGe++OKLUm7z5s3yzDFjxki5pqeIVFRU\n2PLly71r2rdvL+/jb3/7m5R75ZVX/q/ObHoqSUVFhb355pveNZMnT5b3sGjRIim3bt06eeZVV10l\nZ83OncCze/dub27+/PnyzDfeeEPKZWZmyjPDvBdiWrVqZZ07d/bmBg4cKM/861//KuVatmwpz3zv\nvfe8merq6vj1mTNnpNNiKisr5T0cOXJEygVBIM9s1aqVnFXwSREAAIdSBADAoRQBAHAoRQAAHEoR\nAACHUgQAwKEUAQBwKEUAABxKEQAAh1IEAMAJdczb119/bZdddpk3F+Y4skgkIuWysrLkme+//743\nc88998Svk5KSLC0tzbsmzHFRr7/+upRLTU2VZ86ePVvKLVy40MzMzp49a3V1dd78iBEj5D0UFBRI\nuTDHT91yyy1yNqZLly72+OOPe3O33nqrPPPzzz+XckVFRfJM5biupse8devWzd555x3vmnnz5sl7\nGDp0qJRTjiqL6dq1q5w1MyssLLStW7d6cxdccIE8U91vUlKSPLNXr15yNiYvL88mTpzozXXq1Eme\nqR7/mJOTI88M8/M1O3fMm7KPzz77TJ7Z2Ngo5W644QZ55oYNG+Ssgk+KAAA4lCIAAA6lCACAQykC\nAOBQigAAOJQiAAAOpQgAgEMpAgDgUIoAADgJQRDo4YSEMjPTj/P4361TEAS5Zs3uvszcvTXX+zJr\ndj+z5npfZjyLPzTN9b7Mmtzb9wlVigAANGf8+RQAAIdSBADAoRQBAHAoRQAAHEoRAACHUgQAwKEU\nAQBwKEUAABxKEQAAh1IEAMChFAEAcChFAAAcShEAAIdSBADAoRQBAHAoRQAAHEoRAACHUgQAwKEU\nAQBwKEUAAJwWYcLp6elBbm6uN5eUlCTPTEzUevmbb76RZxYWFnozBw8etPLy8gQzszZt2gSdO3f2\nrjl58qS8BzXbsmVLeWYQBFLum2++KQ+CIDc5OTmIRCLefJifV01NjZRrbGyUZ7Zt21bKHT16tDwI\nglwzs4yMjCAvL8+7prKyUt6H+iymp6fLMysqKryZmpoaq6urSzAzS0tLC3JycrxrWrVqJe/hzJkz\nclal3JeZWWVlZXkQBLmtW7cOlNft1KlT8h6U18ks3POdkJAg5fbv3x9/FtG8hCrF3Nxce/rpp705\n9WE109/cY8aMkWdu3brVm+nbt2/8unPnztKazz77TN7Dli1bpJzyiz2mvr5eyo0ePbrIzCwSidig\nQYO8+Wg0Ku/hk08+kXLl5eXyzHHjxkm5mTNnFsWu8/Ly7Pe//713zapVq+R9pKamSrlrr71Wnrli\nxQpv5q233opf5+Tk2COPPOJd06FDB3kPVVVVclal3JeZ2apVq4rMzv1H4pe//KU3r7wPY9TfCWH+\nE5OcnCzlhg0bVuRP4YeIP58CAOBQigAAOJQiAAAOpQgAgEMpAgDgUIoAADiUIgAATqjvKSYmJkrf\naVu3bp0888svv5RyYb4bpn4BN2b37t02cOBAb27q1KnyzOzsbCmXlZUlzzx8+LCcNTv3xfWVK1d6\nc0888YQ8U/mumZnZF198Ic/8/PPP5WxMYmKiKQcTpKWlyTN79uwp5a677jp55qZNm7yZpgc4JCUl\nSd+XvP766+U93HHHHVIuzJf8wx4IEIlErHfv3t5cSUmJPLNbt25S7i9/+Ys8c8SIEXIWzROfFAEA\ncChFAAAcShEAAIdSBADAoRQBAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAAJxQx7zV19fbvn37vLnM\nzEx55tdffy3lwsycMmWKN/Pyyy/Hr6PRqF199dXeNTfccIO8h9mzZ0u5O++8U545c+ZMOWtm1qVL\nF3v66ae9udraWnnmt99+K+VSUlLkmRMnTpRyb7/9dvx6z549duWVV3rXhDlCbsyYMVJO+XdjysrK\nvJnGxsb4dXZ2to0aNcq7ZsaMGfIeHn30USm3du1aeaZ6lOKaNWvMzKy6utrWr1/vzQ8ePFjew4YN\nG6RcmKPbXn31VTmL5olPigAAOJQiAAAOpQgAgEMpAgDgUIoAADiUIgAADqUIAIBDKQIA4FCKAAA4\noU60SUxMtEgk4s0dO3ZMnjly5Egpt2XLFnmmckrM8uXL49dnz561EydOeNcop8PEFBcXS7ndu3fL\nM/fv3y9nzcxKS0ttwYIF3lzHjh3lmcrP38ysRQv90dq1a5ecjTnvvPPsvvvu8+bCnFBy6aWXSrkw\nP7OuXbt6M8nJyfHrmpoae//9971rqqur5T2sXLlSyv34xz+WZ3788cdy1uzc6UpLly715ubMmSPP\nPHjwoJR7/vnn5ZkPPviglJs1a5Y8Ez8sfFIEAMChFAEAcChFAAAcShEAAIdSBADAoRQBAHAoRQAA\nHEoRAACHUgQAwKEUAQBwQh3zdvjwYZs+fbo3N2jQIHlmbm6ulBs4cKA8c9OmTd5M02PdkpKSLD09\n3bvmiy++kPcwYcIEKRfmiLOGhgY5a2YWjUbt6quv9ubCHF/XpUsXKdeuXTt55rRp06TcSy+9FL9W\n762+vl7ex7vvvivlwhyxprwXmt5XY2OjVVRUeNdceeWV8h4OHTok5aLRqDzzueeek7Nm5947CQkJ\n3lyY49PGjRsn5cIcyzd16lQ5i+aJT4oAADiUIgAADqUIAIBDKQIA4FCKAAA4lCIAAA6lCACAQykC\nAOBQigAAOAlBEOjhhIQyMyv6123n36pTEAS5Zs3uvszcvTXX+zJrdj+z5npfZv8fPItoXkKVIgAA\nzRl/PgUAwKEUAQBwKEUAABxKEQAAh1IEAMChFAEAcChFAAAcShEAAIdSBADA+T+L6deV8VN0ZgAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0a9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filter_show(network.params['W1']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ↑ 30개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.29961164386\n",
      "=== epoch:1, train acc:0.217, test acc:0.22 ===\n",
      "train loss:2.29699771561\n",
      "train loss:2.29423688223\n",
      "train loss:2.28717746\n",
      "train loss:2.27999796438\n",
      "train loss:2.26427105331\n",
      "train loss:2.24851614417\n",
      "train loss:2.24709106276\n",
      "train loss:2.21659224761\n",
      "train loss:2.18251402146\n",
      "train loss:2.19536916656\n",
      "train loss:2.12163988723\n",
      "train loss:2.10630952411\n",
      "train loss:2.06697068118\n",
      "train loss:1.98800462153\n",
      "train loss:1.92521156657\n",
      "train loss:1.90635375739\n",
      "train loss:1.8528273125\n",
      "train loss:1.83642710989\n",
      "train loss:1.69479488995\n",
      "train loss:1.5845289591\n",
      "train loss:1.45579951233\n",
      "train loss:1.39028397727\n",
      "train loss:1.27834514764\n",
      "train loss:1.25953951636\n",
      "train loss:1.31556508921\n",
      "train loss:1.18876481897\n",
      "train loss:1.06530012503\n",
      "train loss:1.03656376664\n",
      "train loss:0.916239698969\n",
      "train loss:1.05711002997\n",
      "train loss:0.911143631706\n",
      "train loss:0.799395966614\n",
      "train loss:0.716888138676\n",
      "train loss:0.767050163644\n",
      "train loss:0.698531492328\n",
      "train loss:0.716448121211\n",
      "train loss:0.631013905232\n",
      "train loss:0.660424209895\n",
      "train loss:0.614434846762\n",
      "train loss:0.696908902897\n",
      "train loss:0.691361955407\n",
      "train loss:0.76028579298\n",
      "train loss:0.661694509755\n",
      "train loss:0.51813262941\n",
      "train loss:0.533350018398\n",
      "train loss:0.626050728378\n",
      "train loss:0.666064201127\n",
      "train loss:0.421164065275\n",
      "train loss:0.378548516438\n",
      "train loss:0.652525781298\n",
      "=== epoch:2, train acc:0.824, test acc:0.798 ===\n",
      "train loss:0.59164723598\n",
      "train loss:0.358193323634\n",
      "train loss:0.471253490233\n",
      "train loss:0.461610842521\n",
      "train loss:0.482982247097\n",
      "train loss:0.478796924628\n",
      "train loss:0.523998592987\n",
      "train loss:0.537636551654\n",
      "train loss:0.509064472112\n",
      "train loss:0.400363495234\n",
      "train loss:0.336270372007\n",
      "train loss:0.447742138947\n",
      "train loss:0.357911805098\n",
      "train loss:0.353678818549\n",
      "train loss:0.505735075126\n",
      "train loss:0.325396080366\n",
      "train loss:0.437957389162\n",
      "train loss:0.381192343599\n",
      "train loss:0.534067856284\n",
      "train loss:0.417057227468\n",
      "train loss:0.579935532549\n",
      "train loss:0.313140463842\n",
      "train loss:0.429420482886\n",
      "train loss:0.428798718447\n",
      "train loss:0.342233805199\n",
      "train loss:0.364865163606\n",
      "train loss:0.465689963736\n",
      "train loss:0.310423728929\n",
      "train loss:0.287901963797\n",
      "train loss:0.46370943355\n",
      "train loss:0.383684099888\n",
      "train loss:0.500958528863\n",
      "train loss:0.447490005597\n",
      "train loss:0.345571639247\n",
      "train loss:0.313894292727\n",
      "train loss:0.282171715227\n",
      "train loss:0.395629778056\n",
      "train loss:0.360204934741\n",
      "train loss:0.307803416078\n",
      "train loss:0.388303992282\n",
      "train loss:0.29270593685\n",
      "train loss:0.225363020354\n",
      "train loss:0.41158471989\n",
      "train loss:0.296043926599\n",
      "train loss:0.320548220087\n",
      "train loss:0.391711811806\n",
      "train loss:0.263866725708\n",
      "train loss:0.238421351245\n",
      "train loss:0.176303531471\n",
      "train loss:0.256841872116\n",
      "=== epoch:3, train acc:0.881, test acc:0.861 ===\n",
      "train loss:0.217973457869\n",
      "train loss:0.180354287166\n",
      "train loss:0.465842331563\n",
      "train loss:0.384505700179\n",
      "train loss:0.363760651741\n",
      "train loss:0.263960998753\n",
      "train loss:0.250297272625\n",
      "train loss:0.308448787838\n",
      "train loss:0.229250870288\n",
      "train loss:0.231682318352\n",
      "train loss:0.240480733252\n",
      "train loss:0.300128158627\n",
      "train loss:0.341879583451\n",
      "train loss:0.164492488449\n",
      "train loss:0.195916304344\n",
      "train loss:0.372829224789\n",
      "train loss:0.28874077517\n",
      "train loss:0.272515938364\n",
      "train loss:0.323052002812\n",
      "train loss:0.30395525397\n",
      "train loss:0.275431942872\n",
      "train loss:0.256561636403\n",
      "train loss:0.198624687652\n",
      "train loss:0.295474418654\n",
      "train loss:0.210676193863\n",
      "train loss:0.224500425247\n",
      "train loss:0.506612059791\n",
      "train loss:0.220614450191\n",
      "train loss:0.179263197112\n",
      "train loss:0.278634002537\n",
      "train loss:0.225476023633\n",
      "train loss:0.316987947107\n",
      "train loss:0.454048493283\n",
      "train loss:0.183026194275\n",
      "train loss:0.337293713819\n",
      "train loss:0.483595539642\n",
      "train loss:0.237301116214\n",
      "train loss:0.214408897349\n",
      "train loss:0.359499673453\n",
      "train loss:0.269720811438\n",
      "train loss:0.210129060637\n",
      "train loss:0.273172317389\n",
      "train loss:0.261698185852\n",
      "train loss:0.287972822796\n",
      "train loss:0.238566490456\n",
      "train loss:0.255337695692\n",
      "train loss:0.259020763246\n",
      "train loss:0.413333947061\n",
      "train loss:0.224446621273\n",
      "train loss:0.256131495252\n",
      "=== epoch:4, train acc:0.896, test acc:0.88 ===\n",
      "train loss:0.205444501962\n",
      "train loss:0.20610830411\n",
      "train loss:0.493710660288\n",
      "train loss:0.381921246277\n",
      "train loss:0.39751006413\n",
      "train loss:0.234580146524\n",
      "train loss:0.221101903463\n",
      "train loss:0.218024518606\n",
      "train loss:0.186528956214\n",
      "train loss:0.171339045324\n",
      "train loss:0.201371477068\n",
      "train loss:0.222968556669\n",
      "train loss:0.302332378207\n",
      "train loss:0.11173428756\n",
      "train loss:0.460526378022\n",
      "train loss:0.281063323644\n",
      "train loss:0.276902620138\n",
      "train loss:0.278564174157\n",
      "train loss:0.317727125625\n",
      "train loss:0.231176117771\n",
      "train loss:0.243167303691\n",
      "train loss:0.24826265172\n",
      "train loss:0.22624678404\n",
      "train loss:0.18787600008\n",
      "train loss:0.296341410607\n",
      "train loss:0.24054138371\n",
      "train loss:0.154371441143\n",
      "train loss:0.179798985709\n",
      "train loss:0.207145641225\n",
      "train loss:0.135695826802\n",
      "train loss:0.378797598406\n",
      "train loss:0.155200686082\n",
      "train loss:0.223007087209\n",
      "train loss:0.197020921344\n",
      "train loss:0.314844515159\n",
      "train loss:0.245046598755\n",
      "train loss:0.158407374473\n",
      "train loss:0.31199432243\n",
      "train loss:0.383476455239\n",
      "train loss:0.233154624889\n",
      "train loss:0.121506154495\n",
      "train loss:0.317571429257\n",
      "train loss:0.296813439071\n",
      "train loss:0.142837296422\n",
      "train loss:0.244505913149\n",
      "train loss:0.238502187087\n",
      "train loss:0.314542616524\n",
      "train loss:0.256913110153\n",
      "train loss:0.216285436206\n",
      "train loss:0.278635006298\n",
      "=== epoch:5, train acc:0.902, test acc:0.893 ===\n",
      "train loss:0.178446358549\n",
      "train loss:0.337478583423\n",
      "train loss:0.335581235064\n",
      "train loss:0.277751073713\n",
      "train loss:0.275279649308\n",
      "train loss:0.280111370751\n",
      "train loss:0.347823717312\n",
      "train loss:0.236944945739\n",
      "train loss:0.16746611352\n",
      "train loss:0.20414249153\n",
      "train loss:0.233201980386\n",
      "train loss:0.336514572022\n",
      "train loss:0.137840104831\n",
      "train loss:0.157316993823\n",
      "train loss:0.351228058759\n",
      "train loss:0.28262353161\n",
      "train loss:0.321713250997\n",
      "train loss:0.316467801525\n",
      "train loss:0.187804689437\n",
      "train loss:0.284525754787\n",
      "train loss:0.185394143146\n",
      "train loss:0.230443929279\n",
      "train loss:0.246112832021\n",
      "train loss:0.292371859882\n",
      "train loss:0.326740908604\n",
      "train loss:0.274120719472\n",
      "train loss:0.107484840624\n",
      "train loss:0.238088394139\n",
      "train loss:0.161540942526\n",
      "train loss:0.300740332932\n",
      "train loss:0.275445242781\n",
      "train loss:0.250366042623\n",
      "train loss:0.223840033859\n",
      "train loss:0.183192155733\n",
      "train loss:0.156229584686\n",
      "train loss:0.218137980187\n",
      "train loss:0.19005495417\n",
      "train loss:0.15696118207\n",
      "train loss:0.203031332776\n",
      "train loss:0.153088504557\n",
      "train loss:0.265147369771\n",
      "train loss:0.174814435619\n",
      "train loss:0.182275988925\n",
      "train loss:0.176113887468\n",
      "train loss:0.204139781326\n",
      "train loss:0.285991174859\n",
      "train loss:0.252511466042\n",
      "train loss:0.221980646289\n",
      "train loss:0.115731245776\n",
      "train loss:0.249781851934\n",
      "=== epoch:6, train acc:0.924, test acc:0.905 ===\n",
      "train loss:0.206046155716\n",
      "train loss:0.179832393154\n",
      "train loss:0.212220150445\n",
      "train loss:0.161055766681\n",
      "train loss:0.201061589062\n",
      "train loss:0.16497729301\n",
      "train loss:0.230602527662\n",
      "train loss:0.138045942228\n",
      "train loss:0.245065339179\n",
      "train loss:0.0999600205607\n",
      "train loss:0.180526403126\n",
      "train loss:0.150125568027\n",
      "train loss:0.167359952571\n",
      "train loss:0.169770180854\n",
      "train loss:0.464660328661\n",
      "train loss:0.168986098509\n",
      "train loss:0.2030866881\n",
      "train loss:0.313392203823\n",
      "train loss:0.142682806126\n",
      "train loss:0.309538124723\n",
      "train loss:0.226884809359\n",
      "train loss:0.23318073033\n",
      "train loss:0.204051100958\n",
      "train loss:0.229895404342\n",
      "train loss:0.210013419889\n",
      "train loss:0.197514434009\n",
      "train loss:0.176817942861\n",
      "train loss:0.255256971173\n",
      "train loss:0.213219260203\n",
      "train loss:0.199827757459\n",
      "train loss:0.223883914837\n",
      "train loss:0.182126979998\n",
      "train loss:0.0963340349312\n",
      "train loss:0.265494635757\n",
      "train loss:0.209665439416\n",
      "train loss:0.168622493096\n",
      "train loss:0.327458383688\n",
      "train loss:0.259732070258\n",
      "train loss:0.192000353395\n",
      "train loss:0.191683126969\n",
      "train loss:0.0986729119881\n",
      "train loss:0.194258741623\n",
      "train loss:0.269009179933\n",
      "train loss:0.125130282344\n",
      "train loss:0.178185707289\n",
      "train loss:0.152358518844\n",
      "train loss:0.117234095519\n",
      "train loss:0.200690524873\n",
      "train loss:0.186715501313\n",
      "train loss:0.0711846609529\n",
      "=== epoch:7, train acc:0.929, test acc:0.919 ===\n",
      "train loss:0.195164409498\n",
      "train loss:0.112026671908\n",
      "train loss:0.123231107014\n",
      "train loss:0.243123883909\n",
      "train loss:0.111213325771\n",
      "train loss:0.149379281251\n",
      "train loss:0.184534088521\n",
      "train loss:0.178939189216\n",
      "train loss:0.0852772103331\n",
      "train loss:0.24152604134\n",
      "train loss:0.192253453717\n",
      "train loss:0.126820083292\n",
      "train loss:0.265069677982\n",
      "train loss:0.0887276134935\n",
      "train loss:0.161626007226\n",
      "train loss:0.146176175448\n",
      "train loss:0.269174391752\n",
      "train loss:0.139643540274\n",
      "train loss:0.21027748461\n",
      "train loss:0.103166646302\n",
      "train loss:0.196216613088\n",
      "train loss:0.182088583892\n",
      "train loss:0.174307666393\n",
      "train loss:0.361229648536\n",
      "train loss:0.131019001936\n",
      "train loss:0.107639907073\n",
      "train loss:0.119122738899\n",
      "train loss:0.177664367536\n",
      "train loss:0.16989290615\n",
      "train loss:0.107072540549\n",
      "train loss:0.143706542617\n",
      "train loss:0.155008315654\n",
      "train loss:0.180254397464\n",
      "train loss:0.250415348396\n",
      "train loss:0.168357068462\n",
      "train loss:0.124930454902\n",
      "train loss:0.225891635726\n",
      "train loss:0.175187666561\n",
      "train loss:0.153627439795\n",
      "train loss:0.119277204872\n",
      "train loss:0.189287878608\n",
      "train loss:0.269385869446\n",
      "train loss:0.142046454848\n",
      "train loss:0.222458791197\n",
      "train loss:0.0808067950146\n",
      "train loss:0.130288283712\n",
      "train loss:0.0841902189048\n",
      "train loss:0.200921169932\n",
      "train loss:0.1172537725\n",
      "train loss:0.182488032369\n",
      "=== epoch:8, train acc:0.942, test acc:0.916 ===\n",
      "train loss:0.0953337936251\n",
      "train loss:0.0727621242763\n",
      "train loss:0.22755039968\n",
      "train loss:0.119595154004\n",
      "train loss:0.204776401885\n",
      "train loss:0.0903310367327\n",
      "train loss:0.119729151001\n",
      "train loss:0.183133957007\n",
      "train loss:0.113631900511\n",
      "train loss:0.0871146474488\n",
      "train loss:0.140829437053\n",
      "train loss:0.0682365098189\n",
      "train loss:0.25555475127\n",
      "train loss:0.154873299403\n",
      "train loss:0.148698664869\n",
      "train loss:0.0838136431268\n",
      "train loss:0.0968997526164\n",
      "train loss:0.140718573571\n",
      "train loss:0.171769098835\n",
      "train loss:0.127965338833\n",
      "train loss:0.149129597782\n",
      "train loss:0.188256758583\n",
      "train loss:0.154155267465\n",
      "train loss:0.196598166884\n",
      "train loss:0.140415134246\n",
      "train loss:0.127097183624\n",
      "train loss:0.0585509107646\n",
      "train loss:0.0778309082888\n",
      "train loss:0.163552374654\n",
      "train loss:0.0717713189666\n",
      "train loss:0.148228832493\n",
      "train loss:0.176618486155\n",
      "train loss:0.0942261590166\n",
      "train loss:0.122224049159\n",
      "train loss:0.115157949746\n",
      "train loss:0.190579527119\n",
      "train loss:0.104632550789\n",
      "train loss:0.207005010706\n",
      "train loss:0.146076152998\n",
      "train loss:0.140378573088\n",
      "train loss:0.130367885629\n",
      "train loss:0.113941102181\n",
      "train loss:0.200610144251\n",
      "train loss:0.168596254654\n",
      "train loss:0.13858238326\n",
      "train loss:0.0894117986301\n",
      "train loss:0.0658681479901\n",
      "train loss:0.114583705088\n",
      "train loss:0.0866123519265\n",
      "train loss:0.109214848989\n",
      "=== epoch:9, train acc:0.945, test acc:0.927 ===\n",
      "train loss:0.202246884623\n",
      "train loss:0.151410674809\n",
      "train loss:0.175645509702\n",
      "train loss:0.157656286172\n",
      "train loss:0.124566031171\n",
      "train loss:0.136300640125\n",
      "train loss:0.161020738735\n",
      "train loss:0.0835241359314\n",
      "train loss:0.195952429057\n",
      "train loss:0.150013219914\n",
      "train loss:0.150035916101\n",
      "train loss:0.15191841722\n",
      "train loss:0.171973351416\n",
      "train loss:0.176937189866\n",
      "train loss:0.0511455787107\n",
      "train loss:0.104277034975\n",
      "train loss:0.260177202985\n",
      "train loss:0.193145247153\n",
      "train loss:0.123991076842\n",
      "train loss:0.138016890638\n",
      "train loss:0.097246491626\n",
      "train loss:0.192371437201\n",
      "train loss:0.168529413124\n",
      "train loss:0.116560372871\n",
      "train loss:0.0716647665429\n",
      "train loss:0.158906914635\n",
      "train loss:0.0477231544905\n",
      "train loss:0.094739489987\n",
      "train loss:0.191913462818\n",
      "train loss:0.163645879542\n",
      "train loss:0.0985921435798\n",
      "train loss:0.106726154257\n",
      "train loss:0.114307687915\n",
      "train loss:0.261511411442\n",
      "train loss:0.0796959593022\n",
      "train loss:0.159392836443\n",
      "train loss:0.128717390316\n",
      "train loss:0.140847375555\n",
      "train loss:0.0580207624185\n",
      "train loss:0.231252803135\n",
      "train loss:0.115152413775\n",
      "train loss:0.142156686436\n",
      "train loss:0.132243368198\n",
      "train loss:0.107068878836\n",
      "train loss:0.172787721051\n",
      "train loss:0.119833664471\n",
      "train loss:0.179178212331\n",
      "train loss:0.320379484675\n",
      "train loss:0.165306440531\n",
      "train loss:0.0408585495588\n",
      "=== epoch:10, train acc:0.962, test acc:0.941 ===\n",
      "train loss:0.0910154810898\n",
      "train loss:0.172736165914\n",
      "train loss:0.175662944753\n",
      "train loss:0.151946246821\n",
      "train loss:0.0774177573667\n",
      "train loss:0.205022449255\n",
      "train loss:0.150967799051\n",
      "train loss:0.0954973787969\n",
      "train loss:0.0728577554972\n",
      "train loss:0.0895205706364\n",
      "train loss:0.0683174801955\n",
      "train loss:0.11171112292\n",
      "train loss:0.0747963733519\n",
      "train loss:0.0704625722241\n",
      "train loss:0.112916538574\n",
      "train loss:0.146766214243\n",
      "train loss:0.180233946756\n",
      "train loss:0.120918423112\n",
      "train loss:0.155303352817\n",
      "train loss:0.125763329252\n",
      "train loss:0.113004325363\n",
      "train loss:0.110955328814\n",
      "train loss:0.075588550204\n",
      "train loss:0.175436250839\n",
      "train loss:0.113133449505\n",
      "train loss:0.0893511241353\n",
      "train loss:0.090768442205\n",
      "train loss:0.0716563709897\n",
      "train loss:0.122212658096\n",
      "train loss:0.110127902745\n",
      "train loss:0.125024592545\n",
      "train loss:0.121004134543\n",
      "train loss:0.156889865889\n",
      "train loss:0.0606295074227\n",
      "train loss:0.0505730149876\n",
      "train loss:0.0579576065072\n",
      "train loss:0.0636186772756\n",
      "train loss:0.122663707805\n",
      "train loss:0.0362574154547\n",
      "train loss:0.0982037686603\n",
      "train loss:0.0901771042089\n",
      "train loss:0.0997388819349\n",
      "train loss:0.0432298329476\n",
      "train loss:0.107264747201\n",
      "train loss:0.129665527946\n",
      "train loss:0.140423809085\n",
      "train loss:0.198185780907\n",
      "train loss:0.0503678868779\n",
      "train loss:0.0558210776243\n",
      "train loss:0.0547515737539\n",
      "=== epoch:11, train acc:0.967, test acc:0.939 ===\n",
      "train loss:0.170174629528\n",
      "train loss:0.112176313868\n",
      "train loss:0.0706014307268\n",
      "train loss:0.130929600225\n",
      "train loss:0.073698742432\n",
      "train loss:0.121071930729\n",
      "train loss:0.0520159500198\n",
      "train loss:0.126377066436\n",
      "train loss:0.120123139253\n",
      "train loss:0.0707269015053\n",
      "train loss:0.0871579066794\n",
      "train loss:0.046187137683\n",
      "train loss:0.0675958703852\n",
      "train loss:0.158471209869\n",
      "train loss:0.0847154202416\n",
      "train loss:0.0952231272072\n",
      "train loss:0.208919587693\n",
      "train loss:0.153347374939\n",
      "train loss:0.125769289278\n",
      "train loss:0.0426322908837\n",
      "train loss:0.112100025552\n",
      "train loss:0.0691530112682\n",
      "train loss:0.0689990289839\n",
      "train loss:0.101316161753\n",
      "train loss:0.0429737860828\n",
      "train loss:0.149276737945\n",
      "train loss:0.109348552286\n",
      "train loss:0.0943656584867\n",
      "train loss:0.116859097497\n",
      "train loss:0.0584176336018\n",
      "train loss:0.0663198897495\n",
      "train loss:0.0677125464925\n",
      "train loss:0.106592728638\n",
      "train loss:0.0636328245865\n",
      "train loss:0.0924680540532\n",
      "train loss:0.044631631513\n",
      "train loss:0.0970473518066\n",
      "train loss:0.0418757525969\n",
      "train loss:0.0445493175054\n",
      "train loss:0.156630673875\n",
      "train loss:0.0715107959861\n",
      "train loss:0.156022891992\n",
      "train loss:0.0743617673102\n",
      "train loss:0.0960229040662\n",
      "train loss:0.109978606565\n",
      "train loss:0.0472964795119\n",
      "train loss:0.119017316324\n",
      "train loss:0.0545653701043\n",
      "train loss:0.0701690622171\n",
      "train loss:0.107017070145\n",
      "=== epoch:12, train acc:0.966, test acc:0.934 ===\n",
      "train loss:0.103207806283\n",
      "train loss:0.0373524870352\n",
      "train loss:0.0545748590229\n",
      "train loss:0.0797808374495\n",
      "train loss:0.0358073330965\n",
      "train loss:0.109829707247\n",
      "train loss:0.0587135242596\n",
      "train loss:0.0875514246656\n",
      "train loss:0.143586502718\n",
      "train loss:0.0713601056968\n",
      "train loss:0.0623600578872\n",
      "train loss:0.032249777983\n",
      "train loss:0.158726366025\n",
      "train loss:0.0587481654254\n",
      "train loss:0.0942328940823\n",
      "train loss:0.0429403035823\n",
      "train loss:0.0567632510444\n",
      "train loss:0.0506781788212\n",
      "train loss:0.105064226597\n",
      "train loss:0.054296537028\n",
      "train loss:0.061644989398\n",
      "train loss:0.0653206996343\n",
      "train loss:0.0838636249718\n",
      "train loss:0.100311403042\n",
      "train loss:0.156008863137\n",
      "train loss:0.118273809927\n",
      "train loss:0.0599752507977\n",
      "train loss:0.0362953514062\n",
      "train loss:0.153316018668\n",
      "train loss:0.28171188112\n",
      "train loss:0.0806843529043\n",
      "train loss:0.146883829769\n",
      "train loss:0.0273432744139\n",
      "train loss:0.0330552367451\n",
      "train loss:0.09395758656\n",
      "train loss:0.0657945276306\n",
      "train loss:0.0386751948119\n",
      "train loss:0.0306965928652\n",
      "train loss:0.0734992335834\n",
      "train loss:0.0511845337367\n",
      "train loss:0.0639251788912\n",
      "train loss:0.0486221795665\n",
      "train loss:0.0468151419781\n",
      "train loss:0.118990757142\n",
      "train loss:0.122217074635\n",
      "train loss:0.0734848849836\n",
      "train loss:0.0815503261568\n",
      "train loss:0.0969297374627\n",
      "train loss:0.0333558853593\n",
      "train loss:0.0557031137511\n",
      "=== epoch:13, train acc:0.97, test acc:0.945 ===\n",
      "train loss:0.0656455039239\n",
      "train loss:0.0578690479952\n",
      "train loss:0.104171876599\n",
      "train loss:0.0601077962756\n",
      "train loss:0.0690686563451\n",
      "train loss:0.108312613961\n",
      "train loss:0.0932542671681\n",
      "train loss:0.0438579182085\n",
      "train loss:0.0587261740661\n",
      "train loss:0.0835588482186\n",
      "train loss:0.129238820891\n",
      "train loss:0.0555529599321\n",
      "train loss:0.0443526649901\n",
      "train loss:0.0439217597051\n",
      "train loss:0.0659160484511\n",
      "train loss:0.0433251964949\n",
      "train loss:0.0904836840022\n",
      "train loss:0.174223027259\n",
      "train loss:0.038547614231\n",
      "train loss:0.0541388235094\n",
      "train loss:0.0466580133648\n",
      "train loss:0.0930839745054\n",
      "train loss:0.113420697082\n",
      "train loss:0.0382570569805\n",
      "train loss:0.0682499894738\n",
      "train loss:0.0479185124178\n",
      "train loss:0.0774489695209\n",
      "train loss:0.0360711411027\n",
      "train loss:0.0640066817858\n",
      "train loss:0.0419037387664\n",
      "train loss:0.0553393226805\n",
      "train loss:0.0396018945968\n",
      "train loss:0.103356933274\n",
      "train loss:0.0289191154391\n",
      "train loss:0.0570903758814\n",
      "train loss:0.0463721671397\n",
      "train loss:0.0264209212255\n",
      "train loss:0.0367095247697\n",
      "train loss:0.0482070929904\n",
      "train loss:0.0231430551658\n",
      "train loss:0.0438881173006\n",
      "train loss:0.0532906258073\n",
      "train loss:0.139429198004\n",
      "train loss:0.0329876335187\n",
      "train loss:0.0800419705533\n",
      "train loss:0.0585338981179\n",
      "train loss:0.0461050428792\n",
      "train loss:0.0817446025447\n",
      "train loss:0.0564279314011\n",
      "train loss:0.0307804159762\n",
      "=== epoch:14, train acc:0.977, test acc:0.943 ===\n",
      "train loss:0.052548403006\n",
      "train loss:0.0387628395088\n",
      "train loss:0.0410589100189\n",
      "train loss:0.0525644886031\n",
      "train loss:0.0648751748184\n",
      "train loss:0.047602812954\n",
      "train loss:0.0214641363354\n",
      "train loss:0.0503058872937\n",
      "train loss:0.0367131356822\n",
      "train loss:0.0570468169258\n",
      "train loss:0.0486320287784\n",
      "train loss:0.0394537889327\n",
      "train loss:0.0443064366088\n",
      "train loss:0.135597643921\n",
      "train loss:0.11057015708\n",
      "train loss:0.0352277990602\n",
      "train loss:0.0358223140584\n",
      "train loss:0.0826360467283\n",
      "train loss:0.035822674632\n",
      "train loss:0.106927514059\n",
      "train loss:0.134261274943\n",
      "train loss:0.0778152101608\n",
      "train loss:0.0827721096751\n",
      "train loss:0.101602072803\n",
      "train loss:0.0429080142124\n",
      "train loss:0.0944226053255\n",
      "train loss:0.051427993585\n",
      "train loss:0.103920085928\n",
      "train loss:0.0719217471063\n",
      "train loss:0.0379964711464\n",
      "train loss:0.0810820131118\n",
      "train loss:0.13391475866\n",
      "train loss:0.0721952372113\n",
      "train loss:0.0407142734234\n",
      "train loss:0.0714055557072\n",
      "train loss:0.0518651982021\n",
      "train loss:0.06140568465\n",
      "train loss:0.0730550611517\n",
      "train loss:0.0771230040702\n",
      "train loss:0.0680032519378\n",
      "train loss:0.0687273419188\n",
      "train loss:0.113847759338\n",
      "train loss:0.0412235943972\n",
      "train loss:0.0426989981546\n",
      "train loss:0.105328172386\n",
      "train loss:0.0367667499695\n",
      "train loss:0.0441593430988\n",
      "train loss:0.0942145314849\n",
      "train loss:0.0489920599164\n",
      "train loss:0.108614237642\n",
      "=== epoch:15, train acc:0.98, test acc:0.945 ===\n",
      "train loss:0.0468396265817\n",
      "train loss:0.0427204968449\n",
      "train loss:0.0439496792247\n",
      "train loss:0.0488303861287\n",
      "train loss:0.0558344477066\n",
      "train loss:0.0503922059184\n",
      "train loss:0.104635895333\n",
      "train loss:0.0154845205917\n",
      "train loss:0.0886872702624\n",
      "train loss:0.0565331220951\n",
      "train loss:0.0563359131511\n",
      "train loss:0.0500581296262\n",
      "train loss:0.0300174226759\n",
      "train loss:0.0598747697015\n",
      "train loss:0.0488417079822\n",
      "train loss:0.0961079103352\n",
      "train loss:0.0597514871894\n",
      "train loss:0.0140137624566\n",
      "train loss:0.0465923960235\n",
      "train loss:0.0688496902722\n",
      "train loss:0.031894996568\n",
      "train loss:0.0571588842947\n",
      "train loss:0.0791927693969\n",
      "train loss:0.0817712670785\n",
      "train loss:0.0196792719264\n",
      "train loss:0.0533962885329\n",
      "train loss:0.0115093091948\n",
      "train loss:0.0492993761356\n",
      "train loss:0.0456553695522\n",
      "train loss:0.0365734387278\n",
      "train loss:0.0350956693163\n",
      "train loss:0.0941596493211\n",
      "train loss:0.0382942715581\n",
      "train loss:0.0256126815402\n",
      "train loss:0.0183796491593\n",
      "train loss:0.0250966707435\n",
      "train loss:0.110843164232\n",
      "train loss:0.0701732855716\n",
      "train loss:0.0950827747303\n",
      "train loss:0.075513311014\n",
      "train loss:0.0353559145308\n",
      "train loss:0.109971718765\n",
      "train loss:0.0243384815255\n",
      "train loss:0.0774385759094\n",
      "train loss:0.0495559676072\n",
      "train loss:0.035001618026\n",
      "train loss:0.0516387141469\n",
      "train loss:0.0746290155493\n",
      "train loss:0.0381178370675\n",
      "train loss:0.0356966289627\n",
      "=== epoch:16, train acc:0.986, test acc:0.953 ===\n",
      "train loss:0.0481420367243\n",
      "train loss:0.058200125654\n",
      "train loss:0.0439704991637\n",
      "train loss:0.039287373165\n",
      "train loss:0.0242176372306\n",
      "train loss:0.048885073973\n",
      "train loss:0.0488302377848\n",
      "train loss:0.0700012548209\n",
      "train loss:0.0354769026782\n",
      "train loss:0.0801914154109\n",
      "train loss:0.0724994593754\n",
      "train loss:0.0472297914212\n",
      "train loss:0.075865723204\n",
      "train loss:0.0463378979909\n",
      "train loss:0.0373049143392\n",
      "train loss:0.0708398265836\n",
      "train loss:0.0618272250995\n",
      "train loss:0.0182097431451\n",
      "train loss:0.0751811303722\n",
      "train loss:0.0546382205485\n",
      "train loss:0.0643683889707\n",
      "train loss:0.0817467341369\n",
      "train loss:0.0619835252757\n",
      "train loss:0.0557274331106\n",
      "train loss:0.0553881927032\n",
      "train loss:0.0384437898017\n",
      "train loss:0.0283385102568\n",
      "train loss:0.0743944751358\n",
      "train loss:0.0812963367552\n",
      "train loss:0.037551232058\n",
      "train loss:0.0364382578127\n",
      "train loss:0.0240871114874\n",
      "train loss:0.0201549032092\n",
      "train loss:0.0315473448109\n",
      "train loss:0.0265928181975\n",
      "train loss:0.0405597841024\n",
      "train loss:0.0380156260948\n",
      "train loss:0.0196089031495\n",
      "train loss:0.0868613513013\n",
      "train loss:0.0366535054041\n",
      "train loss:0.0225179470661\n",
      "train loss:0.0301613083822\n",
      "train loss:0.0267051062281\n",
      "train loss:0.0355172530427\n",
      "train loss:0.0307335684838\n",
      "train loss:0.108025620353\n",
      "train loss:0.0242970912127\n",
      "train loss:0.0192558268573\n",
      "train loss:0.0262853080545\n",
      "train loss:0.0311584884132\n",
      "=== epoch:17, train acc:0.986, test acc:0.952 ===\n",
      "train loss:0.0314367744775\n",
      "train loss:0.0499384905901\n",
      "train loss:0.0811231873489\n",
      "train loss:0.0569126961243\n",
      "train loss:0.024985411157\n",
      "train loss:0.0269786543263\n",
      "train loss:0.0417453804328\n",
      "train loss:0.0368881352548\n",
      "train loss:0.0226274965813\n",
      "train loss:0.0155391621677\n",
      "train loss:0.0318716393387\n",
      "train loss:0.0176142461142\n",
      "train loss:0.0600058579499\n",
      "train loss:0.072018407221\n",
      "train loss:0.075265289865\n",
      "train loss:0.0702770885368\n",
      "train loss:0.0216066750903\n",
      "train loss:0.0293976239985\n",
      "train loss:0.0279803908297\n",
      "train loss:0.0535720913668\n",
      "train loss:0.024134973504\n",
      "train loss:0.0488471979223\n",
      "train loss:0.0349275646262\n",
      "train loss:0.117598325155\n",
      "train loss:0.0428581674091\n",
      "train loss:0.0394354277551\n",
      "train loss:0.0298145003854\n",
      "train loss:0.0346065702375\n",
      "train loss:0.0195043773604\n",
      "train loss:0.022761680571\n",
      "train loss:0.0142145826786\n",
      "train loss:0.0268103946898\n",
      "train loss:0.045219823714\n",
      "train loss:0.0383593906912\n",
      "train loss:0.0551913617168\n",
      "train loss:0.0491922415189\n",
      "train loss:0.031941890245\n",
      "train loss:0.0302041846632\n",
      "train loss:0.0480522988035\n",
      "train loss:0.0674622956963\n",
      "train loss:0.0485793687143\n",
      "train loss:0.0365088744531\n",
      "train loss:0.0300945832653\n",
      "train loss:0.0557958220031\n",
      "train loss:0.111188279272\n",
      "train loss:0.0133097585044\n",
      "train loss:0.0599122325652\n",
      "train loss:0.0424299611544\n",
      "train loss:0.0971839146573\n",
      "train loss:0.0294229188999\n",
      "=== epoch:18, train acc:0.986, test acc:0.953 ===\n",
      "train loss:0.0243960149975\n",
      "train loss:0.0269515123973\n",
      "train loss:0.0602090811228\n",
      "train loss:0.0110286291066\n",
      "train loss:0.0337134520509\n",
      "train loss:0.0303434877636\n",
      "train loss:0.0449732650845\n",
      "train loss:0.0194725425039\n",
      "train loss:0.0361692052922\n",
      "train loss:0.0220241711427\n",
      "train loss:0.0556060142429\n",
      "train loss:0.024712517604\n",
      "train loss:0.0173556680381\n",
      "train loss:0.0199363337658\n",
      "train loss:0.0183538262153\n",
      "train loss:0.0390119394892\n",
      "train loss:0.0147840631401\n",
      "train loss:0.0314673472985\n",
      "train loss:0.0231511199551\n",
      "train loss:0.0340959275555\n",
      "train loss:0.0340066504475\n",
      "train loss:0.0160058103227\n",
      "train loss:0.0133546822095\n",
      "train loss:0.027454034566\n",
      "train loss:0.0381682924226\n",
      "train loss:0.0150481286147\n",
      "train loss:0.0231865618977\n",
      "train loss:0.0219459512078\n",
      "train loss:0.00743910558915\n",
      "train loss:0.0514336608729\n",
      "train loss:0.0251408828943\n",
      "train loss:0.0317132960337\n",
      "train loss:0.0159516485172\n",
      "train loss:0.0108479844959\n",
      "train loss:0.0150918916033\n",
      "train loss:0.0260725994906\n",
      "train loss:0.0345533176721\n",
      "train loss:0.0555385966735\n",
      "train loss:0.0484171804192\n",
      "train loss:0.039036104714\n",
      "train loss:0.0281502596995\n",
      "train loss:0.0125647106997\n",
      "train loss:0.0252285150085\n",
      "train loss:0.0394658814897\n",
      "train loss:0.0332180804078\n",
      "train loss:0.0153786320965\n",
      "train loss:0.0062448189073\n",
      "train loss:0.00987357649008\n",
      "train loss:0.00839186215873\n",
      "train loss:0.02282085599\n",
      "=== epoch:19, train acc:0.988, test acc:0.961 ===\n",
      "train loss:0.020169541778\n",
      "train loss:0.0165925100457\n",
      "train loss:0.0115733380617\n",
      "train loss:0.00729688719878\n",
      "train loss:0.0556900853192\n",
      "train loss:0.0263946322314\n",
      "train loss:0.0345199880241\n",
      "train loss:0.0353944628831\n",
      "train loss:0.0328285179517\n",
      "train loss:0.0181725561969\n",
      "train loss:0.0164487633874\n",
      "train loss:0.0200898363216\n",
      "train loss:0.124535958296\n",
      "train loss:0.0203363590899\n",
      "train loss:0.0305300323173\n",
      "train loss:0.0585186413594\n",
      "train loss:0.0279490004291\n",
      "train loss:0.0249479695507\n",
      "train loss:0.0282475520678\n",
      "train loss:0.0773886417809\n",
      "train loss:0.0152601385596\n",
      "train loss:0.066062068277\n",
      "train loss:0.0272900608282\n",
      "train loss:0.0103735771485\n",
      "train loss:0.0271911657318\n",
      "train loss:0.00873706542557\n",
      "train loss:0.0597932974065\n",
      "train loss:0.0602648332718\n",
      "train loss:0.012571242552\n",
      "train loss:0.0303051306008\n",
      "train loss:0.0366922055982\n",
      "train loss:0.0204378997289\n",
      "train loss:0.0607274781378\n",
      "train loss:0.0286072692647\n",
      "train loss:0.036165619211\n",
      "train loss:0.0232899636078\n",
      "train loss:0.0317727770098\n",
      "train loss:0.0175387695186\n",
      "train loss:0.0224336334459\n",
      "train loss:0.0251729912578\n",
      "train loss:0.0147387450004\n",
      "train loss:0.0540105980825\n",
      "train loss:0.0284871461598\n",
      "train loss:0.00602390271761\n",
      "train loss:0.0100654570321\n",
      "train loss:0.0316356597121\n",
      "train loss:0.018261863643\n",
      "train loss:0.0307271700109\n",
      "train loss:0.0324817405016\n",
      "train loss:0.0248837669954\n",
      "=== epoch:20, train acc:0.991, test acc:0.958 ===\n",
      "train loss:0.0212664335737\n",
      "train loss:0.0185548499579\n",
      "train loss:0.0265801083894\n",
      "train loss:0.0281525374836\n",
      "train loss:0.014457074322\n",
      "train loss:0.00970120849096\n",
      "train loss:0.017462589877\n",
      "train loss:0.032492243299\n",
      "train loss:0.0128153217159\n",
      "train loss:0.0212526026573\n",
      "train loss:0.0292407286619\n",
      "train loss:0.0106450081526\n",
      "train loss:0.0173690954564\n",
      "train loss:0.0348929502799\n",
      "train loss:0.0121403572188\n",
      "train loss:0.0207288407018\n",
      "train loss:0.0277186943935\n",
      "train loss:0.0440379079015\n",
      "train loss:0.0193803596992\n",
      "train loss:0.0130724284822\n",
      "train loss:0.0610637010248\n",
      "train loss:0.0164591960281\n",
      "train loss:0.0158432978554\n",
      "train loss:0.0183394591667\n",
      "train loss:0.0271859813644\n",
      "train loss:0.0291515425018\n",
      "train loss:0.0171441060302\n",
      "train loss:0.0167553544481\n",
      "train loss:0.0627424468793\n",
      "train loss:0.0191530962677\n",
      "train loss:0.0223601861488\n",
      "train loss:0.0371222616748\n",
      "train loss:0.0131570975805\n",
      "train loss:0.0193735510374\n",
      "train loss:0.0171360203197\n",
      "train loss:0.015618444217\n",
      "train loss:0.0272183571258\n",
      "train loss:0.0265669243344\n",
      "train loss:0.021626526404\n",
      "train loss:0.0433738995701\n",
      "train loss:0.00773377762589\n",
      "train loss:0.0235861092118\n",
      "train loss:0.0331771335959\n",
      "train loss:0.0114555186079\n",
      "train loss:0.0562164243826\n",
      "train loss:0.00705684926982\n",
      "train loss:0.0175485477048\n",
      "train loss:0.0170988304511\n",
      "train loss:0.0209377627729\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.952\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(network, x_train, t_train, x_test, t_test, # t=target 값 실제값\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEjCAYAAABD3BobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQykCAOD8D4VfSbA3fvDjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xad21f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filter_show(network.params['W1']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8726860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 시각화 하기\n",
    "- 학습 후의 필터에 담긴 정보는 무엇일까?\n",
    "    - 에지: 색상이 바뀐 경계선\n",
    "    - 블롭: 국소적으로 덩어리진 \n",
    "    \n",
    "<img src=\"picture/CNN_EDGE.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 시각화하기\n",
    "- 층 깊이에 따른 추출 정보 변화\n",
    "    - 층이 깊어지면서 더 복잡하고 추상화된 정보가 추출됨\n",
    "    \n",
    "- AlexNet이라는 8계층 CNN의 예\n",
    "\n",
    "<img src=\"picture/AlexNet.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정리\n",
    "- 매개변수 갱신 방법에는 확률적 경사 하강법(SGD) 외에도 모멘텀, AdaGrad, Adam 등이 있다.\n",
    "- CNN은 지금까지의 완전연결 계층 네트워크에 합성곱 계층과 풀링 계층을 새로 추가한다.\n",
    "- 합성곱 계층과 풀링 계층은 im2col(이미지를 행렬로 전개하는 함수)을 이용하면 간단하고 효율적으로 구현할 수 있다.\n",
    "- CNN을 시각화해 보면 계층이 깊어질수록 고급 정보가 추출되는 모습을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 참고자료 (TODO:: 동영상&자료 정리!! )\n",
    "- 간단한 설명보면, 재귀적으로 표현되지 노드가! t는 time이다. t+1상태 h(결과)를 뽑을 때 이전 상태(t상태)의 것을 고려해서 결과값을 출력하겠다!! \n",
    "- RNN은 memory 기능이 있다! \n",
    "- CNN에서는 서로 독립적이지..\n",
    "- short term => 현재 / long term => 과거 \n",
    "- 이 RNN에 어디에 쓰이나.. **자연어처리**, 번역, 연관검색어\n",
    "- t를 꼭 시간으로만 보는 것은 아니다. 이전글자 참고하기 ex) 글자자동완성부분, 다음글자 뭔지.. \n",
    "- state의 weight는 다 똑같다. 그래서 뒤로갈수록 0에 수렴해 버리지... 이것을 보완한 것이 LSTN이다!! \n",
    "\n",
    "---\n",
    "\n",
    "[Reference] TODO::정리!!\n",
    "  * 김 성훈 교수님 유투브 강의 ML lab12 시리즈\n",
    "    - https://www.youtube.com/watch?v=B5GtZuUvujQ\n",
    "  * 테리 엄태웅 유투브 강의\n",
    "    - https://www.youtube.com/watch?v=SoNtAjxA3Jo\n",
    "  * Colah's blog\n",
    "    - http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "    - 번역: http://www.whydsp.org/280\n",
    "  * Andrej Karpathy's blog\n",
    "    - http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
