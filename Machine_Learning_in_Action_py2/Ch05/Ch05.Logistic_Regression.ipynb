{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://wikidocs.net/4267 참고함!\n",
    "# 03. Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Linear regression 이 주어진 feature에 따라 continous한 target 값을 추정하는 방법이었다면, classification은 주어진 feature에 따라 데이터를 discrete한 class에 분류하는 방법이다.\n",
    "\n",
    "예를 들면 받은 e-mail이 스팸인지 아닌지, 어떤 종양이 양성인지 악성인지 등을 판별하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification에 선형을 사용하는 데는 문제가 있다. target $y$ 는 0 또는 1의 값만 갖는다. 그런데 linear regression의 $h_\\theta (x)$ 는 1보다 크거나 0보다 작은 값을 내보낼 수 있다.\n",
    "\n",
    "따라서 Logistic regression은 hypothesis function이 0과 1 사이의 값만 내보내도록 한다. Logistic regression은 이름에 regression이 들어있긴 하지만 classifier라는 것을 기억하도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Hypothesis Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression 의 hypothesis function은 0과 1 사이의 값만 내보내도록 하려고 한다. 즉, $ 0 \\leq h_\\theta(x) \\leq 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logreg201.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 함수는 'sigmoid function' 또는 'logistic function' 이라고 불린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of Hypothesis Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y=1$ (class 1)일 확률과 $y=0$\n",
    "(class 0)일 확률은 합이 1이 되어야 한다는 사실을 기억해두자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ P(y=0| x;\\theta) + P(y=1| x; \\theta) = 1 $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "규칙은 간단하다. Hypothesis function이 0.5가 넘으면 Class 1에, 0.5 가 되지 않으면 Class 0에 넣기로 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose predict\n",
    "\n",
    "$\n",
    "y=\n",
    "\\begin{cases}\n",
    "1 & \\text{if } h_\\theta(x) \\ge 0.5, z\\ge0 \\\\\n",
    "0 & \\text{if } h_\\theta(x) < 0.5, z<0 \n",
    "\\end{cases}\n",
    "$\n",
    "- z = 0 가 기준선인 거지!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 hypothesis function $h_\\theta(x)=g(x)$이 0.5 이상이 되는 경우는 $z=\\theta^{T}x\\geq0$ 일 때이다. 그리고 0.5 이하가 되는 경우는 $\\theta^{T}x<0$일 때이다. 즉, $\\theta^{T}x=0$ 일 때를 기준으로 class 가 나뉘게 된다.\n",
    "\n",
    "$\n",
    "y=\n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\theta^T x \\ge 0 \\\\\n",
    "0 & \\text{if } \\theta^T x < 0 \n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가령 다음과 같은 data를 가지고 있을 때, 모종의 방법으로 최적의 parameter $θ$ 를 찾았다고 하자.\n",
    "\n",
    "- 위 그림 z 공간에서 실제 feature 공간에서는 어떻게 분류되는지 궁금해서 그려보면 아래와 같다!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logreg303.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 최적의 parameter를 가지고 다음과 같은 결론을 내릴 수 있다.\n",
    "\n",
    "Predict $y=1$ if\n",
    "\n",
    "$ \\underbrace{-3 + x_1 + x_2 }_{ \\color{royalblue}{\\theta^T x}} \\ge 0 $\n",
    "- ↑ z=0 이 기준이기 때문에!! \n",
    "\n",
    "$ i.e. x_1 +x_2 \\ge 3 $ \n",
    "\n",
    "So, Decision boundary:\n",
    "\n",
    "$ h_\\theta (x) = 0.5 $ i.e. $\\color{royalblue}{\\theta^T x}=0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, $\\color{royalblue}{\\theta^{T}x} = -3 + x_1 + x_2 \\geq 0$ 이면 Class 1 ($y=1$)에 넣고 그렇지 않으면 Class 0 ($y=0$)에 넣는다. 이 때의 decision boundary는 $-3+x_1+x_2=0$ 이다. 즉, 위 그림에서 녹색으로 표시된 선에 해당한다. 이제 training set에 들어있지 않았던, 새로운 데이터가 나타나면 $x1$ 값과 $x2$ 값을 이용하여 좌표를 찍어보고, decision boundary보다 위쪽에 찍히면 class 1에, 아래쪽에 찍히면 class 0에 넣으면 되는 것이다.\n",
    "\n",
    "\n",
    "이 때, **decision boundary**는 $θ$ 에 의해 결정되는 것임을 기억하자. Training data는 parameter를 결정하는 데에 이용될 뿐, decision boundary 에 직접적으로 영향을 미치지는 않는다.\n",
    "\n",
    "eg) x와 o가 트레이닝 데이터 인데,, 그것들이 decision boundary를 결정하지 않지 $\\theta$가 결정한다!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Decision Boundaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logreg305.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Cost Function & Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression의 cost function의 LSE criterion을 그대로 사용하면 logsitic regression 의 cost function은 non-convex function이 된다. 그래서 logistic regression의 cost funciton은 조금 다른 형태를 갖는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Cost Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(log를 씌우는 이유는 울퉁불퉁한 그래프의 local minimum을 없애고 매끄럽게 만들기 위해서 이다!!, convex 만들기 위해서 log를 취함!! )\n",
    "<img src=\"lec_05_08.png\" />\n",
    "- http://pythonkim.tistory.com/22 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression 의 cost function을 다음과 같이 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\text{cost}\\left( h_\\theta (x), y \\right) = \n",
    "\\begin{cases}\n",
    "-\\log(h_\\theta (x)) & \\text{if }  y=1 \\\\\n",
    "-\\log(1-h_\\theta (x)) & \\text{if } y=0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "$y=1$ 일 때 cost function 의 특징은,\n",
    "- $cost = 0$ when $h_\\theta (x) =1$ \n",
    "- $\\text{cost} \\to \\infty$ as $h_\\theta (x) \\to 0$\n",
    "\n",
    "예측을 잘 못한 경우 penalty를 주는 거다. 정리하자면 다음과 같다!\n",
    "\n",
    "$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\text{cost}\\left( h_\\theta ( x^{(i)}), y^{(i)} \\right)\n",
    "$\n",
    "\n",
    "where, $ \\text{cost}\\left( h_\\theta (x), y \\right) = \n",
    "\\begin{cases}\n",
    "-\\log(h_\\theta (x)) & \\text{if }  y=1 \\\\\n",
    "-\\log(1-h_\\theta (x)) & \\text{if } y=0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "- 간단하게 표현하면\n",
    "\n",
    "$\n",
    "\\text{cost} \\left( h_\\theta (x), y  \\right) =\n",
    "\\color{royalblue}{-} y     \\color{royalblue}{\\log\\left( h_\\theta (x) \\right)} \n",
    "\\color{salmon}{-} (1-y)    \\color{salmon}{\\log\\left( 1-h_\\theta (x) \\right)}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "\\because\n",
    "& & \\text{if } y=1, \\text{cost}(h,y) = \\color{royalblue}{-\\log\\left( h_\\theta (x) \\right)}  \\\\\n",
    "& & \\text{if } y=0, \\text{cost}(h,y) = \\color{salmon}   {-\\log\\left( 1- h_\\theta (x) \\right)}\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "최종적으로, logistic regression의 cost function은 다음과 같이 표현한다.\n",
    "\n",
    "$\n",
    "J(\\theta) = - \\frac{1}{m} \n",
    "\\sum_{i=1}^{m} \\left[ \n",
    "y^{(i)} \\log h_\\theta (x) +\n",
    "(1-y^{(i)}) \\log \\left( 1-h_\\theta (x) \\right)\n",
    "\\right]\n",
    "$\n",
    "\n",
    "이 cost function의 특징은,\n",
    "1. Maximum likelihood estimation criterion\n",
    "2. **Convex**\n",
    "\n",
    "이제 최적의 prameter $\\theta$를 찾으려면 $\\min_{\\theta} J(\\theta)$를 구하고, 그 후에 주어진 새로운 $x$를 어떤 class에 넣을지 판단하려면 $h_{\\theta}(x)=\\frac{1}{1+\\exp(-\\theta^{T}x)}$가 0.5보다 큰지 작은지를 확인하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "J(\\theta) = - \\frac{1}{m} \n",
    "\\sum_{i=1}^{m} \\left[ \n",
    "y^{(i)} \\log h_\\theta (x) +\n",
    "(1-y^{(i)}) \\log \\left( 1-h_\\theta (x) \\right)\n",
    "\\right]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 cost function을 최소화하는 parameter $θ$를 찾는 것이 목적이다. $J(θ)$가 convex이므로, gradient descent에 의해 optimal $θ$를 찾을 수 있다. 즉,\n",
    "\n",
    "Repeat{ \n",
    "\n",
    "$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) $\n",
    "\n",
    "} simultaneously update all $\\theta_j$\n",
    "\n",
    "이 때,\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n",
    "= \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$\n",
    "\n",
    "이 수식만 보면 linear regression의 gradient descent와 동일하다. 유일한 차이점은 hypothesis function이 $h_\\theta (x) = \\frac{1}{1+\\exp(-\\theta^{T} x)}$ 로 바뀐 것이다. \n",
    "\n",
    "(이 cost function은 non-convex 돼서 다른 cost function 만든 거 아니었음??)\n",
    "\n",
    "=> log를 씌워줘서 convex를 만들어 줬을 뿐이다 cost function 그래서 다른 cost function이라고 한거임\n",
    "\n",
    "=> hypothesis function과 cost function을 헷갈리면 안된다!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Advanced Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent 외에도 optimization algorithm이 여럿 있다. 예를 들면, conjugate gradient, BFGS, L-BFGS 등이다.\n",
    "\n",
    "이 알고리즘의 장점은 learning rate αα 를 자동으로 골라주며, gradient descent보다 빠른 경우가 많다는 점인데, 복잡하고 이해하기 어렵다.\n",
    "\n",
    "이 강의에서는 이러한 optimization 알고리즘들에 관해 자세히 다루지는 않고, 간략히 소개만 해주었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logreg703.PNG\" />"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Action Ch05 봄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch05. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General approach to logistic regression (로지스틱 회귀의 일반적인 접근 방법)\n",
    "1. Collect: \n",
    "    - Any method.\n",
    "2. Prepare: \n",
    "    - Numeric values are needed for a distance calculation. A structured data format is best.\n",
    "3. Analyze: \n",
    "    - Any method.\n",
    "4. Train: \n",
    "    - We’ll spend most of the time training, where we try to find optimal coefficients to classify our data.\n",
    "5. Test: \n",
    "    - Classification is quick and easy once the training step is done.\n",
    "6. Use: \n",
    "    - This application needs to get some input data and output structured numeric values. Next, the application applies the simple regression calculation on this input data and determines which class the input data should belong to. The application then takes some action on the calculated class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Classification with logistic regression and the sigmoid function: a tractable step function\n",
    "\n",
    "- Logistic regression\n",
    "- Pros: Computationally inexpensive, easy to implement, knowledge representation easy to interpret\n",
    "- Cons: Prone to underfitting, may have low accuracy \n",
    "- Works with: Numeric values, nominal values\n",
    "\n",
    "\n",
    "- sigmoid = logistic / unit step function(단위 계단 함수) = heaviside function\n",
    "- n 차원은 항상 n-1 차원으로 나눠진다! => 이 n-1차원을 hyperplane(초평명) 이라 한다!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Using optimization to find the best regression coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Gradient ascent\n",
    "책은 ascent네, 주로 descent를 사용 했는데!! 하지만 원리는 가다고 보면 된다.\n",
    "\n",
    "ascent이기 때문에 위로 올라가지 + 임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Train: using gradient ascent to find the best parameters\n",
    "#### Listing 5.1 Logistic regression gradient ascent optimization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def loadDataSet():\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open('testSet.txt')\n",
    "    for line in fr.readlines():\n",
    "        lineArr = line.strip().split()\n",
    "        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])]) \n",
    "        # x0 x1 x2   독립변수 들어감! \n",
    "        labelMat.append(int(lineArr[2]))\n",
    "        # 1,0,1,0,,, 종속변수 들어감! \n",
    "    return dataMat,labelMat\n",
    "\n",
    "def sigmoid(inX): # inX에 z를 넣을 것이다 이는 t(W)X 임! Transposed_W * X\n",
    "    # 100x1\n",
    "    return 1.0/(1+exp(-inX))\n",
    "\n",
    "def gradAscent(dataMatIn, classLabels): # 100x3, 1x100\n",
    "    dataMatrix = mat(dataMatIn)             #convert to NumPy matrix (행렬로 변환)\n",
    "    # 100 x 3\n",
    "    labelMat = mat(classLabels).transpose() #convert to NumPy matrix\n",
    "    # 100 x 1\n",
    "    m,n = shape(dataMatrix) # matrix 행, 열 반환!! \n",
    "    alpha = 0.001\n",
    "    maxCycles = 500\n",
    "    weights = ones((n,1)) # 3 x 1\n",
    "    for k in range(maxCycles):              #heavy on matrix operations\n",
    "        h = sigmoid(dataMatrix*weights)     #matrix mult\n",
    "        error = (labelMat - h)              #vector subtraction\n",
    "                 #100x1\n",
    "        # **Gradiant descent algorithm 적용부분!!**\n",
    "\n",
    "        weights = weights + alpha * dataMatrix.transpose()* error #matrix mult\n",
    "        # 3 x 1                       # 3 x 100           # 100x1\n",
    "        # ascent 니까 weight에 +해주지!! \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://stackoverflow.com/questions/22594063/what-is-the-difference-between-gradient-descent-and-gradient-ascent/22594406#22594406\n",
    "- weights 변경 식 설명 참고 link ↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logRegres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataArr, labelMat = logRegres.loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, -0.017612, 14.053064], [1.0, -1.395634, 4.662541], [1.0, -0.752157, 6.53862], [1.0, -1.322371, 7.152853], [1.0, 0.423363, 11.054677], [1.0, 0.406704, 7.067335], [1.0, 0.667394, 12.741452], [1.0, -2.46015, 6.866805], [1.0, 0.569411, 9.548755], [1.0, -0.026632, 10.427743], [1.0, 0.850433, 6.920334], [1.0, 1.347183, 13.1755], [1.0, 1.176813, 3.16702], [1.0, -1.781871, 9.097953], [1.0, -0.566606, 5.749003], [1.0, 0.931635, 1.589505], [1.0, -0.024205, 6.151823], [1.0, -0.036453, 2.690988], [1.0, -0.196949, 0.444165], [1.0, 1.014459, 5.754399], [1.0, 1.985298, 3.230619], [1.0, -1.693453, -0.55754], [1.0, -0.576525, 11.778922], [1.0, -0.346811, -1.67873], [1.0, -2.124484, 2.672471], [1.0, 1.217916, 9.597015], [1.0, -0.733928, 9.098687], [1.0, -3.642001, -1.618087], [1.0, 0.315985, 3.523953], [1.0, 1.416614, 9.619232], [1.0, -0.386323, 3.989286], [1.0, 0.556921, 8.294984], [1.0, 1.224863, 11.58736], [1.0, -1.347803, -2.406051], [1.0, 1.196604, 4.951851], [1.0, 0.275221, 9.543647], [1.0, 0.470575, 9.332488], [1.0, -1.889567, 9.542662], [1.0, -1.527893, 12.150579], [1.0, -1.185247, 11.309318], [1.0, -0.445678, 3.297303], [1.0, 1.042222, 6.105155], [1.0, -0.618787, 10.320986], [1.0, 1.152083, 0.548467], [1.0, 0.828534, 2.676045], [1.0, -1.237728, 10.549033], [1.0, -0.683565, -2.166125], [1.0, 0.229456, 5.921938], [1.0, -0.959885, 11.555336], [1.0, 0.492911, 10.993324], [1.0, 0.184992, 8.721488], [1.0, -0.355715, 10.325976], [1.0, -0.397822, 8.058397], [1.0, 0.824839, 13.730343], [1.0, 1.507278, 5.027866], [1.0, 0.099671, 6.835839], [1.0, -0.344008, 10.717485], [1.0, 1.785928, 7.718645], [1.0, -0.918801, 11.560217], [1.0, -0.364009, 4.7473], [1.0, -0.841722, 4.119083], [1.0, 0.490426, 1.960539], [1.0, -0.007194, 9.075792], [1.0, 0.356107, 12.447863], [1.0, 0.342578, 12.281162], [1.0, -0.810823, -1.466018], [1.0, 2.530777, 6.476801], [1.0, 1.296683, 11.607559], [1.0, 0.475487, 12.040035], [1.0, -0.783277, 11.009725], [1.0, 0.074798, 11.02365], [1.0, -1.337472, 0.468339], [1.0, -0.102781, 13.763651], [1.0, -0.147324, 2.874846], [1.0, 0.518389, 9.887035], [1.0, 1.015399, 7.571882], [1.0, -1.658086, -0.027255], [1.0, 1.319944, 2.171228], [1.0, 2.056216, 5.019981], [1.0, -0.851633, 4.375691], [1.0, -1.510047, 6.061992], [1.0, -1.076637, -3.181888], [1.0, 1.821096, 10.28399], [1.0, 3.01015, 8.401766], [1.0, -1.099458, 1.688274], [1.0, -0.834872, -1.733869], [1.0, -0.846637, 3.849075], [1.0, 1.400102, 12.628781], [1.0, 1.752842, 5.468166], [1.0, 0.078557, 0.059736], [1.0, 0.089392, -0.7153], [1.0, 1.825662, 12.693808], [1.0, 0.197445, 9.744638], [1.0, 0.126117, 0.922311], [1.0, -0.679797, 1.22053], [1.0, 0.677983, 2.556666], [1.0, 0.761349, 10.693862], [1.0, -2.168791, 0.143632], [1.0, 1.38861, 9.341997], [1.0, 0.317029, 14.739025]]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print dataArr  # 독립변수\n",
    "print labelMat # 종속변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'logRegres' from 'logRegres.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(logRegres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00  -1.76120000e-02   1.40530640e+01]\n",
      " [  1.00000000e+00  -1.39563400e+00   4.66254100e+00]\n",
      " [  1.00000000e+00  -7.52157000e-01   6.53862000e+00]\n",
      " [  1.00000000e+00  -1.32237100e+00   7.15285300e+00]\n",
      " [  1.00000000e+00   4.23363000e-01   1.10546770e+01]\n",
      " [  1.00000000e+00   4.06704000e-01   7.06733500e+00]\n",
      " [  1.00000000e+00   6.67394000e-01   1.27414520e+01]\n",
      " [  1.00000000e+00  -2.46015000e+00   6.86680500e+00]\n",
      " [  1.00000000e+00   5.69411000e-01   9.54875500e+00]\n",
      " [  1.00000000e+00  -2.66320000e-02   1.04277430e+01]\n",
      " [  1.00000000e+00   8.50433000e-01   6.92033400e+00]\n",
      " [  1.00000000e+00   1.34718300e+00   1.31755000e+01]\n",
      " [  1.00000000e+00   1.17681300e+00   3.16702000e+00]\n",
      " [  1.00000000e+00  -1.78187100e+00   9.09795300e+00]\n",
      " [  1.00000000e+00  -5.66606000e-01   5.74900300e+00]\n",
      " [  1.00000000e+00   9.31635000e-01   1.58950500e+00]\n",
      " [  1.00000000e+00  -2.42050000e-02   6.15182300e+00]\n",
      " [  1.00000000e+00  -3.64530000e-02   2.69098800e+00]\n",
      " [  1.00000000e+00  -1.96949000e-01   4.44165000e-01]\n",
      " [  1.00000000e+00   1.01445900e+00   5.75439900e+00]\n",
      " [  1.00000000e+00   1.98529800e+00   3.23061900e+00]\n",
      " [  1.00000000e+00  -1.69345300e+00  -5.57540000e-01]\n",
      " [  1.00000000e+00  -5.76525000e-01   1.17789220e+01]\n",
      " [  1.00000000e+00  -3.46811000e-01  -1.67873000e+00]\n",
      " [  1.00000000e+00  -2.12448400e+00   2.67247100e+00]\n",
      " [  1.00000000e+00   1.21791600e+00   9.59701500e+00]\n",
      " [  1.00000000e+00  -7.33928000e-01   9.09868700e+00]\n",
      " [  1.00000000e+00  -3.64200100e+00  -1.61808700e+00]\n",
      " [  1.00000000e+00   3.15985000e-01   3.52395300e+00]\n",
      " [  1.00000000e+00   1.41661400e+00   9.61923200e+00]\n",
      " [  1.00000000e+00  -3.86323000e-01   3.98928600e+00]\n",
      " [  1.00000000e+00   5.56921000e-01   8.29498400e+00]\n",
      " [  1.00000000e+00   1.22486300e+00   1.15873600e+01]\n",
      " [  1.00000000e+00  -1.34780300e+00  -2.40605100e+00]\n",
      " [  1.00000000e+00   1.19660400e+00   4.95185100e+00]\n",
      " [  1.00000000e+00   2.75221000e-01   9.54364700e+00]\n",
      " [  1.00000000e+00   4.70575000e-01   9.33248800e+00]\n",
      " [  1.00000000e+00  -1.88956700e+00   9.54266200e+00]\n",
      " [  1.00000000e+00  -1.52789300e+00   1.21505790e+01]\n",
      " [  1.00000000e+00  -1.18524700e+00   1.13093180e+01]\n",
      " [  1.00000000e+00  -4.45678000e-01   3.29730300e+00]\n",
      " [  1.00000000e+00   1.04222200e+00   6.10515500e+00]\n",
      " [  1.00000000e+00  -6.18787000e-01   1.03209860e+01]\n",
      " [  1.00000000e+00   1.15208300e+00   5.48467000e-01]\n",
      " [  1.00000000e+00   8.28534000e-01   2.67604500e+00]\n",
      " [  1.00000000e+00  -1.23772800e+00   1.05490330e+01]\n",
      " [  1.00000000e+00  -6.83565000e-01  -2.16612500e+00]\n",
      " [  1.00000000e+00   2.29456000e-01   5.92193800e+00]\n",
      " [  1.00000000e+00  -9.59885000e-01   1.15553360e+01]\n",
      " [  1.00000000e+00   4.92911000e-01   1.09933240e+01]\n",
      " [  1.00000000e+00   1.84992000e-01   8.72148800e+00]\n",
      " [  1.00000000e+00  -3.55715000e-01   1.03259760e+01]\n",
      " [  1.00000000e+00  -3.97822000e-01   8.05839700e+00]\n",
      " [  1.00000000e+00   8.24839000e-01   1.37303430e+01]\n",
      " [  1.00000000e+00   1.50727800e+00   5.02786600e+00]\n",
      " [  1.00000000e+00   9.96710000e-02   6.83583900e+00]\n",
      " [  1.00000000e+00  -3.44008000e-01   1.07174850e+01]\n",
      " [  1.00000000e+00   1.78592800e+00   7.71864500e+00]\n",
      " [  1.00000000e+00  -9.18801000e-01   1.15602170e+01]\n",
      " [  1.00000000e+00  -3.64009000e-01   4.74730000e+00]\n",
      " [  1.00000000e+00  -8.41722000e-01   4.11908300e+00]\n",
      " [  1.00000000e+00   4.90426000e-01   1.96053900e+00]\n",
      " [  1.00000000e+00  -7.19400000e-03   9.07579200e+00]\n",
      " [  1.00000000e+00   3.56107000e-01   1.24478630e+01]\n",
      " [  1.00000000e+00   3.42578000e-01   1.22811620e+01]\n",
      " [  1.00000000e+00  -8.10823000e-01  -1.46601800e+00]\n",
      " [  1.00000000e+00   2.53077700e+00   6.47680100e+00]\n",
      " [  1.00000000e+00   1.29668300e+00   1.16075590e+01]\n",
      " [  1.00000000e+00   4.75487000e-01   1.20400350e+01]\n",
      " [  1.00000000e+00  -7.83277000e-01   1.10097250e+01]\n",
      " [  1.00000000e+00   7.47980000e-02   1.10236500e+01]\n",
      " [  1.00000000e+00  -1.33747200e+00   4.68339000e-01]\n",
      " [  1.00000000e+00  -1.02781000e-01   1.37636510e+01]\n",
      " [  1.00000000e+00  -1.47324000e-01   2.87484600e+00]\n",
      " [  1.00000000e+00   5.18389000e-01   9.88703500e+00]\n",
      " [  1.00000000e+00   1.01539900e+00   7.57188200e+00]\n",
      " [  1.00000000e+00  -1.65808600e+00  -2.72550000e-02]\n",
      " [  1.00000000e+00   1.31994400e+00   2.17122800e+00]\n",
      " [  1.00000000e+00   2.05621600e+00   5.01998100e+00]\n",
      " [  1.00000000e+00  -8.51633000e-01   4.37569100e+00]\n",
      " [  1.00000000e+00  -1.51004700e+00   6.06199200e+00]\n",
      " [  1.00000000e+00  -1.07663700e+00  -3.18188800e+00]\n",
      " [  1.00000000e+00   1.82109600e+00   1.02839900e+01]\n",
      " [  1.00000000e+00   3.01015000e+00   8.40176600e+00]\n",
      " [  1.00000000e+00  -1.09945800e+00   1.68827400e+00]\n",
      " [  1.00000000e+00  -8.34872000e-01  -1.73386900e+00]\n",
      " [  1.00000000e+00  -8.46637000e-01   3.84907500e+00]\n",
      " [  1.00000000e+00   1.40010200e+00   1.26287810e+01]\n",
      " [  1.00000000e+00   1.75284200e+00   5.46816600e+00]\n",
      " [  1.00000000e+00   7.85570000e-02   5.97360000e-02]\n",
      " [  1.00000000e+00   8.93920000e-02  -7.15300000e-01]\n",
      " [  1.00000000e+00   1.82566200e+00   1.26938080e+01]\n",
      " [  1.00000000e+00   1.97445000e-01   9.74463800e+00]\n",
      " [  1.00000000e+00   1.26117000e-01   9.22311000e-01]\n",
      " [  1.00000000e+00  -6.79797000e-01   1.22053000e+00]\n",
      " [  1.00000000e+00   6.77983000e-01   2.55666600e+00]\n",
      " [  1.00000000e+00   7.61349000e-01   1.06938620e+01]\n",
      " [  1.00000000e+00  -2.16879100e+00   1.43632000e-01]\n",
      " [  1.00000000e+00   1.38861000e+00   9.34199700e+00]\n",
      " [  1.00000000e+00   3.17029000e-01   1.47390250e+01]]\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "100 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 4.12414349],\n",
       "        [ 0.48007329],\n",
       "        [-0.6168482 ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logRegres.gradAscent(dataArr, labelMat)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "matrix([[ 4.12414349],    # W0\n",
    "        [ 0.48007329],    # W1\n",
    "        [-0.6168482 ]])   # W2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) $\n",
    "\n",
    "$ 0 \\leq h_\\theta(x) \\leq 1 $\n",
    "\n",
    "${\\theta^{T}x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ h_w(z) = \\frac{1}{1+exp(-z)}$\n",
    "\n",
    "$z = W^{T}X = [w_0 w_1 w_2][x_0 x_1 x_2]^{T} \\\\ (x_0 = 1) $\n",
    "\n",
    "$w_0 + w_1x_1 + x_2w_2 = 4.12 + 0.48x_1 - 0.61x_2 = 0$\n",
    "\n",
    "인 $x_1, x_2$ 로 구성된 차원의 선을 그릴 수 있지 그 Decision Boundary data들이 분리되는 것이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotBestFit(weights):\n",
    "    import matplotlib.pyplot as plt\n",
    "    dataMat,labelMat=loadDataSet()\n",
    "    dataArr = array(dataMat)\n",
    "    n = shape(dataArr)[0] \n",
    "    xcord1 = []; ycord1 = []\n",
    "    xcord2 = []; ycord2 = []\n",
    "    for i in range(n):\n",
    "        if int(labelMat[i])== 1:\n",
    "            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])\n",
    "        else:\n",
    "            xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')\n",
    "    ax.scatter(xcord2, ycord2, s=30, c='green')\n",
    "    x = arange(-3.0, 3.0, 0.1)\n",
    "    y = (-weights[0]-weights[1]*x)/weights[2]\n",
    "    ax.plot(x, y)\n",
    "    plt.xlabel('X1'); plt.ylabel('X2');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'logRegres' from 'logRegres.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(logRegres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = logRegres.gradAscent(dataArr, labelMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhIIEocIiISiBKHiIgE8v8Bsop6nrlyQAkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7d16198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logRegres.plotBestFit(weights.getA())\n",
    "# getA는 array로 변환해주는 함수!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
